# 上层提交bio请求

Linux 通用块层提供给上层的接口函数是 submit_bio。上层在构造好 bio 请求之后，调用 submit_bio 提交给 Linux 通用块层处理。

submit_bio 函数是上层向通用块层提交 I/O 请求的通用接口。它包含两个参数：第一个为 I/O 请求方式，为读／写，或者预读；第二个为描述通用块层 I/O 请求的描述符。

```c
void submit_bio(int rw, struct bio *bio)
{
	bio->bi_rw |= rw;

	/*
	 * If it's a regular read/write or a barrier with data attached,
	 * go through the normal accounting stuff before submission.
	 */
	if (bio_has_data(bio)) {
		unsigned int count;

		if (unlikely(rw & REQ_WRITE_SAME))
			count = bdev_logical_block_size(bio->bi_bdev) >> 9;
		else
			count = bio_sectors(bio);

		if (rw & WRITE) {
			count_vm_events(PGPGOUT, count);
		} else {
			task_io_account_read(bio->bi_size);
			count_vm_events(PGPGIN, count);
		}

		if (unlikely(block_dump)) {
			char b[BDEVNAME_SIZE];
			printk(KERN_DEBUG "%s(%d): %s block %Lu on %s (%u sectors)\n",
			current->comm, task_pid_nr(current),
				(rw & WRITE) ? "WRITE" : "READ",
				(unsigned long long)bio->bi_sector,
				bdevname(bio->bi_bdev, b),
				count);
		}
	}

	generic_make_request(bio);
}
EXPORT_SYMBOL(submit_bio);
```

中间大段的代码对于有数据传输的读／写或屏障请求的统计代码，如果不考虑，submit_bio 就剩下generic_make_request 函数，这个函数执行全部的工作。

为了防止栈式块设备执行请求可能出现问题，在一个时刻只允许进程有一个 generic_make _request 被调用。为此，在进程结构中定义了一个 bio 等待处理链表： bio_list。同时区分活动和非活动状态。活动状态表示当前进程已经在调用 generic_make_request 中。这时，所有后续产生的 bio 都被链入 bio_list 链表，在当前 bio 处理完成的情况下，顺序逐个处理。

generic_make_request 的执行过程是：

-   首先判断 make_request 是否处于活动状态。如果 current->bio_list 不为NULL，则表明当前进程已经有 generic_make_request 函数在执行，这时候我们将传入的 bio 链接到当前进程等待处理的 bio 链表，返回。如果 current->bio_list 为NULL，则跳转到第 2 步；
-   设置 current->bio_list 表明当前 generic_make_request 正处于活动状态，让后来的 bio 有机会插入到等待链表；
-   处理 bio。需要注意的是，这里处理的 bio 可能是传入的 bio，也可能是当前进程待处理 bio 链表中的 bio。如果是前者，上层保证了其 bi_next 必然为 NULL；如果是后者，则在将 bio 从链表中脱链时，已经设置了其 bi_next 为 NULL；
-   回调 make_request_fn 处理这个 bio；

具体流程分析见注释：

```c
void generic_make_request(struct bio *bio)
{
	/*
	 * bio_list_on_stack[0] 包含被当前 make_request_fn 提交的 bio。
	 * bio_list_on_stack[1] 包含在当前 make_request_fn 之前提交但尚未处理的 bio。
	 */
	struct bio_list bio_list_on_stack[2];
	struct request_queue *q = bdev_get_queue(bio->bi_bdev);

	if (blk_queue_enter(q, 0) < 0) {
		bio_io_error(bio);
		return;
	}

	/*
	 * 检查请求是否超过限制，
	 * 将分区请求映射到磁盘上，映射之后工作都是在整个磁盘上进行。
	 * 检查一些特殊请求标记如 DISCARD
	 */
	if (!generic_make_request_checks(bio))
		goto out;

	/*
	 * 我们只希望同一时间只有一个 ->make_request_fn 处于活动状态，
	 * 否则堆栈设备的堆栈使用可能是一个问题。 
	 * 所以使用 current->bio_list 来保存由 make_request_fn 函数提交的请求列表。 
	 * current->bio_list 也被用作一个标志来表示在这个任务中, generic_make_request 当前是否处于活动状态。 
	 * 如果它为 NULL，则没有 make_request 处于活动状态。 
	 * 如果它是非 NULL，那么 make_request 是活动的，并且应该在尾部添加新的请求
	 */
	if (current->bio_list) {
		bio_list_add(&current->bio_list[0], bio);
		goto out;
	}

	/* 
	 * 在进入循环之前， bio->bi_next 为 NULL（因为所有调用者都确保这一点），
	 * 所以我们有一个包含单个 bio 的列表。
	 * 我们假设我们刚刚从一个更长的列表中取出它，
	 * 所以我们将 bio_list 分配给指向 bio_list_on_stack，
	 * 从而初始化要添加的新 bios 的 bio_list。 
	 * ->make_request() 确实可以通过对 generic_make_request 的递归调用添加更多的 bios。 
	 * 如果是这样，我们在 bio_list 中找到一个非 NULL 值并从顶部重新进入循环。 
	 * 在这种情况下，我们确实只是获取了列表顶部的 bio（不假装），
	 * 因此将其从 bio_list 中删除，并再次调用 ->make_request()。
	 */
	BUG_ON(bio->bi_next);
	bio_list_init(&bio_list_on_stack[0]);
	current->bio_list = bio_list_on_stack;
	
	do {
		bool enter_succeeded = true;

		if (unlikely(q != bdev_get_queue(bio->bi_bdev))) {
			if (q)
				blk_queue_exit(q);
			q = bdev_get_queue(bio->bi_bdev);
			if (blk_queue_enter(q, 0) < 0) {
				enter_succeeded = false;
				q = NULL;
			}
		}

		if (enter_succeeded) {
			struct bio_list lower, same;

			/* Create a fresh bio_list for all subordinate requests */
			bio_list_on_stack[1] = bio_list_on_stack[0];
			bio_list_init(&bio_list_on_stack[0]);
			/*
			 * 所有块设备都需要分配一个requst_queue描述符。
			 * 对于SCSI磁盘设备，这比较容易理解，
			 * 因为它们需要一个请求队列来记录“排序、合并”后的请求；
			 * 而MD、Device Mapper等设备并不真正使用请求队列，
			 * 它需要request_queue的原因是利用其中包含的make_request_fn方法来
			 * 完成对bio请求的“重定向”。
			 */
			q->make_request_fn(q, bio);

			/* sort new bios into those for a lower level
			 * and those for the same level
			 */
			bio_list_init(&lower);
			bio_list_init(&same);
			while ((bio = bio_list_pop(&bio_list_on_stack[0])) != NULL)
				if (q == bdev_get_queue(bio->bi_bdev))
					bio_list_add(&same, bio);
				else
					bio_list_add(&lower, bio);
			/* now assemble so we handle the lowest level first */
			bio_list_merge(&bio_list_on_stack[0], &lower);
			bio_list_merge(&bio_list_on_stack[0], &same);
			bio_list_merge(&bio_list_on_stack[0], &bio_list_on_stack[1]);
		} else {
			bio_io_error(bio);
		}
		bio = bio_list_pop(&bio_list_on_stack[0]);
	} while (bio);
	current->bio_list = NULL; /* deactivate */
 out:
	if (q)
		blk_queue_exit(q);
}
```



无论是物理块设备，还是逻辑块设备，都有一个请求队列，它的地址是对应通用磁盘描述符的 queue 域（即 bio->bi_bdev->bd_disk->queue）。request_queue 包含的 make_request_fn 方法来实现请求的提交。



---

# 构造request请求

Linux 块设备驱动程序可以分为三类，分别针对顺序访问物理设备、随机访问物理设备和逻辑设备（即栈式设备）。三类块设备的请求队列的情况如表所示。

| 类型                       | make_request_fn                               | request_fn      | 备注                                                         |
| :------------------------- | :-------------------------------------------- | :-------------- | ------------------------------------------------------------ |
| SCSI 设备等                | 从 bio 构造 request（经过合并和排序），返回 0 | 逐个处理request | 调用 blk_init_queue，使用默认的 __make_request，提供策略例程 |
| SSD 等                     | 直接处理 bio，返回 0                          | 无              | 调用 blk_alloc_queue，提供 make_request_fn（调用 blk_queue_make_request 或直接设置） |
| RAID 或 Device Mapper 设备 | 重定向 bio，返回非零值                        | 无              | 调用 blk_alloc_queue，提供 make_request_fn（调用 blk_queue_make_request 或直接设置） |

第一类是使用请求队列的块设备驱动。来自上层的 I/O 到达块设备层后，经过 I/O 调度器的排序、合并，转换为请求加入到请求队列 request_queue 中，而上述排序、合并过程是在 make_request 函数中完成的。与此相对应，请求的处理是由策略例程完成的，这就是函数 request_fn 要做的。它进而又分为两种：

其一是非中断驱动的，策略例程的处理过程是：逐个将请求队列中的元素取出，通知块设备控制器处理该请求，等待数据传输完成，之后继续下一个请求。

其二是中断驱动的，策略例程的处理过程是：策略例程传输队列中第一个请求的数据，同时设置块设备控制器，让它在数据传输完成时产生一个中断。由中断处理函数再次激活策略例程，或者为当前请求启动另一次数据传输，或者在当前请求结束后，将它从请求队列中取出，并继续处理下一个请求。

第二类和第三类块设备驱动程序并不真正使用请求队列，所以它们都不需要策略例程，不过它们还是需要 request_queue 结构和 make_request 函数，只是 make_request 并不负责排序、合并、加入到请求队列的操作，而是执行两种不同的处理。

第二类块设备驱动直接执行请求，向上次报告请求已经执行完以及执行的结果。如果说第一类块设备驱动程序常常用在磁盘类块设备中，那么这一种直接处理的块设备驱动程序便应用于真正意义的随机访问块设备，例如 ramdisk，它们不需要考虑为提高磁头移动性能而做优化的种种方法。

第三类块设备驱动的处理方式是重定向请求，常见于**栈式**块设备驱动，例如 RAID 和 Device Mapper，它修改请求设备号以及请求扇区号后，返回非零值。块设备层判断该工作尚未完成并且需要重试。但是，这次重试是在底层块设备的 request_queue 结构和 make_request 函数上进行的。

不论中间经过多少层的逻辑设备，bio 请求最终将到达低层的 SCSI 设备。其 request_queue 在初始化 gendisk 结构时即被指向对应的 scsi_device 结构的 request_queue（参见 sd_probe 函数）。而后者是在探测到 scsi_device，在 scsi_alloc_sdev 函数中调用 scsi_alloc_queue 实现的，scsi_alloc_queue 进而调用 blk_init_queue，而 blk_init_queue 是为希望使用标准的请求处理过程，即排序和合并请求的块设备提供的。scsi_alloc_queue 最终分配 request_queue 描述符，并初始化几个主要的回调函数，包括：

-   将 request_fn 回调函数初始化为 scsi_request_fn；
-   将 make_request_fn 回调函数初始化为 blk_queue_bio；
-   将 prep_rq_fn 回调函数初始化为 scsi_prep_fn；
-   将 softirq_done_fn 回调函数初始化为 scsi_softirq_done；
-   将 rq_timed_out_fn 回调函数初始化为 scsi_times_out。

设置代码如下：

```c
struct request_queue *scsi_alloc_queue(struct scsi_device *sdev)
{
	struct request_queue *q;

	q = __scsi_alloc_queue(sdev->host, scsi_request_fn);
	if (!q)
		return NULL;

	blk_queue_prep_rq(q, scsi_prep_fn);				// q->prep_rq_fn = scsi_prep_fn;
	blk_queue_unprep_rq(q, scsi_unprep_fn);
	blk_queue_softirq_done(q, scsi_softirq_done);	// q->softirq_done_fn = scsi_softirq_done;
	blk_queue_rq_timed_out(q, scsi_times_out);		// q->rq_timed_out_fn = scsi_times_out;
	blk_queue_lld_busy(q, scsi_lld_busy);
	return q;
}

struct request_queue *__scsi_alloc_queue(struct Scsi_Host *shost,
					 request_fn_proc *request_fn)
{
	struct request_queue *q;

	q = blk_init_queue(request_fn, NULL);
	...
	__scsi_init_queue(shost, q);
	return q;
}

struct request_queue *blk_init_queue(request_fn_proc *rfn, spinlock_t *lock)
{
	return blk_init_queue_node(rfn, lock, NUMA_NO_NODE);
}

struct request_queue *
blk_init_queue_node(request_fn_proc *rfn, spinlock_t *lock, int node_id)
{
	struct request_queue *q;

	q = blk_alloc_queue_node(GFP_KERNEL, node_id, lock);
	...

	q->request_fn = rfn;	// q->request_fn = scsi_request_fn
	if (blk_init_allocated_queue(q) < 0) {
		blk_cleanup_queue(q);
		return NULL;
	}

	return q;
}

int blk_init_allocated_queue(struct request_queue *q)
{
	...
        
	blk_queue_make_request(q, blk_queue_bio);	// 默认绑定 blk_queue_bio 来处理 IO

	...
}


```

上级提交的请求会使用 q->make_request_fn 回调函数来处理，对于 SCSI 设备来说，默认设置为 blk_queue_bio。

blk_queue_bio 首先尝试创建**反弹缓冲区**（blk_queue_bounce），通常是在驱动尝试在外围设备不可达到的地址，例如高端内存上执行 DMA 时。它会逐个 segment 检查原始 bio，如果所有页面都落在 DMA 内存范围之内，则直接返回，这时候不需要反弹；否则，说明有一个或多个 segment 使用的页面在 DMA 内存范围之外，就需要反弹。这个时候我们就需要分配能执行操作的内存，如果是写操作，要把 bio 的数据复制过去，如果是读，则在操作完成之后复制过来。

所谓反弹，实际上是分配一个新的 bio 描述符，它和原始 bio 的 segment 一一对应。如果原始 bio 的 segment 使用的页面在 DMA 内存范围之外，则分配一个在 DMA 内存范围之内的页面，赋给新的 bio 对应的 segment。对于写操作，还需要将前一页面的内容复制到后一页面。如果原始 bio 的 segment 使用的页面在 DMA 内存范围之内，则将新的 bio 对应的 segment 也指向同一个页面。

最后，将原始 bio 保存在新的 bio 的 bi_private 域，并且设置新的 bio 的 I/O 结束函数（bi_end_io 域）为bounce_end_io_xxx 系列函数之一（如 bounce_end_io_write 或者 bounce_end_io_read）。这样，就可通过第二个参数返回新的 bio 描述符，这也就是它使用使用双重指针的原因。

在 I/O 结束后，bounce_end_io_xxx 回调函数会被调用，这些函数同样在文件 mm/bounce.c 中，它们进行一些相关的处理，比如读操作时将读到反弹缓冲区的数据复制到目标缓冲区（它可以根据两个 bio 的对应 segment 是否指向同一个页面来判断这个 segment 是否使用反弹缓冲区），以及释放反弹缓冲区等，之后，再调用原始 bio（保存在 bi_private 域）的 I/O 结束回调函数。

所以，如果反弹缓冲区创建成功，则接下来处理新的 bio，而不是原始 bio。

​	

接下来，请求处理进入关键一环，即交给 I/O 调度器，由它负责**合并和排序请求**。合并，是指将对磁盘上连续位置的请求合并为一个，通过一次 SCSI 命令完成。排序是将多个请求对磁盘上的访问位置顺序重新排列，使得磁头尽可能地向一个方向移动。请求的合并和排序是在 SCSI 设备的请求队列描述符上进行的。

顾名思义，请求队列本身会将请求组织成一个队列，但这并不是唯一的队列，严格地讲，它是 SCSI 设备的派发队列，也是请求合并和排序之后的队列。除此之外，请求队列还有一个或多个 I/O 调度队列，这取决于 I/O 调度器的类型。从上层发过来的请求首先放入 I/O 调度器的队列，通过某种特定的调度算法，依次送到派发队列中，由 SCSI 设备的策略例程逐个处理。

请求是否能够合并，或者是否可以排序，取决于多种因素。例如，当请求已经超过硬件的最大限制时，就不能再合并新的请求。再如，屏障 I/O 之前和之后的请求，就不允许放在一起排序。

elv_merge 函数判断 bio 是否可以和请求队列中的某个 request 进行合并，函数有三种可能的返回值——向前合并、向后合并和无法合并。

如果电梯算法确定可以将 bio 合并到现有的 request 中，它返回值 ELEVATOR_BACK_MERGE 或 ELEVATOR_FRONT_MERGE。它还给出要将 bio 添加到其中的 request 的指针。返回值表明要将 bio 添加到 request 的哪个位置（向后合并表明 bio 应该被添加到 request 的 bio 链表的尾部）。

-   ELEVATOR_BACK_MERGE 表示 bio 可以合并到某个 request 后面，它还需要检查是否可以在这个 request 执行向后合并，这通常校验合并是否会导致违背请求的最大扇区数目等限制。电梯算法只告诉说这个 request 可以被扩大，它并不知道块设备驱动程序对请求是否有限制。因此，这些回调函数允许块设备驱动程序控制合并操作。bio_attempt_back_merge 进行检查后将 bio 插入到 request 的 bio 链表的尾部，同时递增该 request 的数据长度；如果 request 还没有设置亲近的 cpu，则采用 bio 的设置。这次合并可能正好填补 request 和起始位置上后一个 request 之间的空洞。若是这样，attempt_back_merge 进一步合并这两个 request。
-   ELEVATOR_FRONT_MERGE 表示 bio 可以合并到某个 request 前面，bio_attempt_front_merge 确保块设备驱动允许这次合并，并将 bio 插入到 request 的 bio 链表的头部。和后部合并不同的是，除了数据长度外，request 的起始位置也发生了变化。按同样的方式修改 cpu，同时，这次合并可能正好填补 request 和起始位置上前一个 request 之间的空洞。若是这样，attempt_front_merge 进一步合并这两个 request。
-   最后一种情况是返回 ELEVATOR_NO_MERGE，即在请求队列中找不到可以合并的 request。这时候就需要为 bio 申请一个新的 request。get_request 保证能够成功分配，若当前没有空闲的 request 描述符，则函数挂起当前进程，直到有请求处理完成之后释放出来。

最终，我们可以获得一个 request 描述符，init_request_from_bio 根据 bio 对它进行初始化，并添加到 I/O 调度器队列。所做的工作包括：

-   置请求的命令类型为 REQ_TYPE_FS，表示这是来自上层的请求；
-   设置请求的各种标志，如是否是软屏障、硬屏障、同步请求等；
-   清零请求的错误计数器；
-   设置请求的起始扇区编号；
-   计算请求的以字节为单位的数据长度；
-   将 bio 插入 request 链表；
-   设置请求所对应的通用磁盘描述符。

详细分析见注释：

```c
void blk_queue_bio(struct request_queue *q, struct bio *bio)
{
	const bool sync = !!(bio->bi_rw & REQ_SYNC);
	struct blk_plug *plug;
	int el_ret, rw_flags, where = ELEVATOR_INSERT_SORT;
	struct request *req, *free;
	unsigned int request_count = 0;

	/*
	 * 创建一个反弹缓冲区。
	 * 通常是在驱动尝试在外围设备不可达到的地址，例如高端内存上执行 DMA 时。
	 * 
	 * 所谓反弹，实际上是分配一个新的 bio 描述符，它和原始 bio 的 segment 一一对应。
	 * 如果原始 bio 的 segment 使用的页面在 DMA 内存范围之外，
	 * 则分配一个在 DMA 内存范围之内的页面，赋给新的 bio 对应的 segment。
	 * 对于写操作，还需要将前一页面的内容复制到后一页面。
	 * 如果原始 bio 的 segment 使用的页面在 DMA 内存范围之内，则将新的 bio 对应的 segment 也指向同一个页面。
	 * 最后，将原始 bio 保存在新的 bio 的 bi_private 域，
	 * 并且设置新的 bio 的 I/O 结束函数（bi_end_io 域）为 bounce_end_io_xxx 系列函数之一。
	 * 这样，就可通过第二个参数返回新的 bio 描述符，这也就是它使用使用双重指针的原因。
	 * 
	 * 在 I/O 结束后，bounce_end_io_xxx 回调函数会被调用，它们进行一些相关的处理，
	 * 比如读操作时将读到反弹缓冲区的数据复制到目标缓冲区，以及释放反弹缓冲区等，
	 * 之后，再调用原始 bio（保存在 bi_private 域）的 I/O 结束回调函数。
	 * 所以，如果反弹缓冲区创建成功，则接下来处理新的 bio，而不是原始 bio。
	 */
	blk_queue_bounce(q, &bio);

	if (bio_integrity_enabled(bio) && bio_integrity_prep(bio)) {
		bio_endio(bio, -EIO);
		return;
	}

	if (bio->bi_rw & (REQ_FLUSH | REQ_FUA)) {
		spin_lock_irq(q->queue_lock);
		where = ELEVATOR_INSERT_FLUSH;
		goto get_rq;
	}

	/*
	 * 每个进程有一个 plug 队列，IO 先尝试插入该队列，在队列满时再 flush 到请求队列，
	 * 这样可以避免频繁对请求队列操作导致的锁竞争，提升效率。
	 */
	if (!blk_queue_nomerges(q)) {
		if (blk_attempt_plug_merge(q, bio, &request_count, NULL))
			return;
	} else
		request_count = blk_plug_queued_count(q);

	// 如果无法合并至进程的 plug_list,只能乖乖插入 request_queue 了，进行加锁
	spin_lock_irq(q->queue_lock);

	/*
	 * IO 调度器，由它负责合并和排序请求
	 */
	el_ret = elv_merge(q, &req, bio);
	if (el_ret == ELEVATOR_BACK_MERGE) {
		/*
		 * ELEVATOR_BACK_MERGE 表示 bio 可以合并到某个 request 后面，
		 * 它还需要检查是否可以在这个 request 执行向后合并，
		 * 这通常校验合并是否会导致违背请求的最大扇区数目等限制。
		 * 电梯算法只告诉说这个 request 可以被扩大，它并不知道块设备驱动程序对请求是否有限制。
		 * 因此，这些回调函数允许块设备驱动程序控制合并操作。
		 * bio_attempt_back_merge 进行检查后将 bio 插入到 request 的 bio 链表的尾部，
		 * 同时递增该 request 的数据长度；
		 * 如果 request 还没有设置亲近的 cpu，则采用 bio 的设置。
		 * 这次合并可能正好填补 request 和起始位置上后一个 request 之间的空洞。
		 * 若是这样，attempt_back_merge 进一步合并这两个 request。
		 */
		if (bio_attempt_back_merge(q, req, bio)) {
			elv_bio_merged(q, req, bio);
			free = attempt_back_merge(q, req);
			if (!free)
				elv_merged_request(q, req, el_ret);
			else
				__blk_put_request(q, free);
			goto out_unlock;
		}
	} else if (el_ret == ELEVATOR_FRONT_MERGE) {
		/*
		 * LEVATOR_FRONT_MERGE 表示 bio 可以合并到某个 request 前面，
		 * bio_attempt_front_merge 确保块设备驱动允许这次合并，
		 * 并将 bio 插入到 request 的 bio 链表的头部。
		 * 和后部合并不同的是，除了数据长度外，request 的起始位置也发生了变化。
		 */
		if (bio_attempt_front_merge(q, req, bio)) {
			elv_bio_merged(q, req, bio);
			free = attempt_front_merge(q, req);
			if (!free)
				elv_merged_request(q, req, el_ret);
			else
				__blk_put_request(q, free);
			goto out_unlock;
		}
	}

get_rq:
	/*
	 * sync 标志检查在 init_request_from_bio() 会再做一次,
	 * 但是我们需要提前暴露设置给队列分配器和IO调度器
	 */
	rw_flags = bio_data_dir(bio);
	if (sync)
		rw_flags |= REQ_SYNC;

	/*
	 * 申请一个新的 request，一定会成功，拿不到就睡眠
	 */
	blk_queue_enter_live(q);
	req = get_request(q, rw_flags, bio, 0);
	if (IS_ERR(req)) {
		blk_queue_exit(q);
		bio_endio(bio, PTR_ERR(req));	/* @q is dead */
		goto out_unlock;
	}

	/*
	 * 根据 bio 初始化新的 request。
	 * 经过睡眠之后现在请求可能是能被合并的，我们不太担心这种情况
	 * 它不经常发生，而且电梯算法能处理它
	 */
	init_request_from_bio(req, bio);

	if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags))
		req->cpu = raw_smp_processor_id();

	plug = current->plug;
	if (plug) {
		/*
		 * If this is the first request added after a plug, fire
		 * of a plug trace.
		 *
		 * @request_count may become stale because of schedule
		 * out, so check plug list again.
		 */
		if (!request_count || list_empty(&plug->list))
			trace_block_plug(q);
		else {
			struct request *last = list_entry_rq(plug->list.prev);
			// 积攒请求一把 flush 下去
			if (request_count >= BLK_MAX_REQUEST_COUNT ||
			    blk_rq_bytes(last) >= BLK_PLUG_FLUSH_SIZE) {
			    // false 表示同步 plug
				blk_flush_plug_list(plug, false);
				trace_block_plug(q);
			}
		}
		// 将新请求加入 plug
		list_add_tail(&req->queuelist, &plug->list);
		blk_account_io_start(req, true);
	} else {
		spin_lock_irq(q->queue_lock);
		// 没有 plug 就只能加锁放入 request_queue
		add_acct_request(q, req, where);
		__blk_run_queue(q);
out_unlock:
		spin_unlock_irq(q->queue_lock);
	}
}
EXPORT_SYMBOL_GPL(blk_queue_bio);	/* for device mapper only */
```



---

# 蓄流泄流

很长时间以来，Linux 块设备层使用了一种称为蓄流／泄流（Plugging/Unplugging）的技术来改进吞吐率。简单而言，蓄流／泄流的工作方式类似于浴盆排水系统的塞子——当 I/O 被提交（相当于水流入）时，它被储存在一个队列（相当于蓄水池，这时塞子是被堵住的）。稍后的某个时间，我们才允许 I/O 从队列中派发出去（相当于将塞子拔开，允许水流出）。前面的过程称为蓄流，而后面的过程称为泄流。

块 I/O 子系统之所以不将每个来自上层的 I/O 请求立即派发到低层，允许少量 I/O 的累计，是为了实现 I/O 的调度。简单地讲，它为可能的 I/O 合并和排序创造机会。合并是将对磁盘连续扇区访问的多个 I/O 合并为一个 I/O，而排序是将 I/O 按照对磁盘访问的扇区编号进行排序，尽量使得磁头向一个方向移动。事实证明，这种处理方式极大地改进了对基于磁盘的块设备访问的性能。



## 2.6内核实现

Linux 块 I/O 层为请求队列的蓄流和泄流分别提供了一系列公共函数。这里介绍 blk_plug_device 和 generic_unplug_device。

blk_plug_device 函数为块设备，或更精确地说，是块设备的请求队列蓄流。这个函数以请求队列描述符的地址为参数，设置 queue_flags 域的 QUEUE_FLAG_PLUGGED 标志，然后启动运行在 unplug_timer 域中的泄流定时器。

在调用 blk_plug_device 函数蓄流设备的请求队列时，为它设置 QUEUE_FLAG_PLUGGED 标志，更新为当前时刻后的某个时间，具体值取决于请求队列描述符的 unplug_delay 域的值，在 blk_queue_make_request 中被设置为 3 ms。

超时处理函数在 blk_queue_make_request 中被设置为 blk_unplug_timeout。在这个函数中，服务 kblockd_workqueue 工作队列的 kblockd 内核线程被唤醒。这个内核线程执行其地址保存在 q->unplug_work 中的函数，也就是 blk_unplug_work，这是在 blk_alloc_queue_node 函数中设置的。这个函数调用请求队列的 unplug_fn 方法，通常被实例化为 generic_unplug_device 函数。

generic_unplug_device 函数负责泄流块设备：首先，它检查是否请求队列还处于活动状态；然后，调用清除请求队列的蓄流标志，删除泄流定时器；最后，执行策略例程 request_fn 方法开始处理队列中的下一个请求。

此外，如果队列中等待处理的请求数超过了保存在请求队列描述符中的 unplug_thresh 域的值（默认为 4），则 I/O 调度器为请求队列泄流。在 elv_insert 函数中，如果出现这种情况，会调用 generic_unplug_device 函数。generic_unplug_device 是 generic_unplug_device 的简单封装，主要是确保请求队列处于蓄流状态，并让后者在自旋锁的保护下进行。

在 2.6 内核中，泄流的时机有两个：

-   定时器超时了，开始泄流，这是应对请求较少场景下 request 不会被饿死
-   请求队列中的请求数超过一定量，默认为 4，开始泄流，这是为了应对突发请求量较大的场景

那什么时候开始蓄流呢?

-   第一次向该设备提交请求时要蓄流，因为此时该设备的 request_queue 是空的；
-   如果泄流完成或者泄流过程中发现底层设备已经疲于应付（发送请求返回错误了），主动退出泄流模式，进入蓄流状态。这是非常合理的，因为底层设备有处理能力限制，而且上层是异步发送，我们不能不管底层设备的死活。

如果在泄流的时候上层又下发了 bio，怎么办呢？等泄流完成吗?

底层泄流的过程其实是很快的，因为每个请求发下去并给它一个回调函数就可以了，无需等着它完成。而且在泄流过程中，在从 request_queue 中获取到一个 request 后，就会解除 request_queue 的 lock，这也就意味着文件系统可以向该块设备层继续提交新请求。

存在的问题：

-   加锁解锁的地方太多，很容易影响性能，虽然用的是自旋锁。且整个地方只有一把大锁
-   如果在泄流的过程中上层（文件系统）源源不断地发送请求的话，可能达不到蓄流的效果，上层提交的过快，而泄流线程可能没那么快，导致的结果就是来一个 request 就泄掉，再来一个还是泄掉，前后 request 无法做合并和排序，影响性能。



## 3.10内核改进

好，那既然上面说到的蓄流泄流算法存在种种的弊端，那我们如何改进？让我们对症下药，针对上面的症结提出应对之策：

1.   化整为零：细化锁粒度；
2.   批量提交：上层文件系统提交的时候不要一次一个来，太费事儿，一次给我来一打吧。

新版内核中细化了锁的粒度，除了 request_queue 全局有一把大锁以外，每个进程增加了一个 plug_list ，这样，在极大程度上可以实现真正的并行了：当 IO 请求提交时，首先插入该队列，在 plug_list 满时，再 flush 到设备的请求队列 request_queue 中，这样可避免频繁对设备的请求队列操作导致的锁竞争，提升效率。

plug_list 结构如下。

```c
struct blk_plug {
	unsigned long magic; /* detect uninitialized use-cases */
	struct list_head list; /* requests */
	struct list_head mq_list; /* blk-mq requests */
	struct list_head cb_list; /* md requires an unplug callback */
};
```

蓄流和泄流接口分别为 blk_start_plug 和 blk_finish_plug，这两个函数一般成对出现，我们先来看一下 blk_start_plug：

```c
void blk_start_plug(struct blk_plug *plug)
{
	struct task_struct *tsk = current;

	plug->magic = PLUG_MAGIC;
	INIT_LIST_HEAD(&plug->list);
	INIT_LIST_HEAD(&plug->mq_list);
	INIT_LIST_HEAD(&plug->cb_list);

	if (!tsk->plug) {
		tsk->plug = plug;
	}
}
```

该函数功能比较简单，在创建并初始化一个 plug 之后将其添加到当前进程描述符 current 的 plug 字段中，后续该进程访问 plug_list 都是通过访问 current->plug 的方式实现的。接着我们来分析一下泄流操作：

```c
void blk_finish_plug(struct blk_plug *plug)
{
	blk_flush_plug_list(plug, false);

	if (plug == current->plug)
		current->plug = NULL;
}
```

从代码中我们可以看到泄流操作调用了 blk_flush_plug_list 将 plug 中的 request 泄流到调度队列中，并设置 current->plug 字段为 NULL。



我们回去看看 blk_queue_bio 的代码。在提交给 IO 调度器之前调用了 blk_attempt_plug_merge 尝试将 bio 合并到当前 plug_list 中。如果合入失败，则只能乖乖加锁进行 request_queue 操作。

```c
bool blk_attempt_plug_merge(struct request_queue *q, struct bio *bio,
			    unsigned int *request_count,
			    struct request **same_queue_rq)
{
	struct blk_plug *plug;
	struct request *rq;
	bool ret = false;
	struct list_head *plug_list;

	// 找到该进程的 plug 队列
	plug = current->plug;
	if (!plug)
		goto out;
	*request_count = 0;

	if (q->mq_ops)
		plug_list = &plug->mq_list;
	else
		plug_list = &plug->list;

	/*
	 * 遍历队列里的每个 request，检查 bio 是否可以合并至该 request，条件有：
	 * - bio 和 request 属于同一个设备（queue 一致）
	 * - io 请求连续
	 * - 合并后的 request 内 IO 请求大小未超过硬件限制
	 */
	list_for_each_entry_reverse(rq, plug_list, queuelist) {
		int el_ret;

		if (rq->q == q) {
			(*request_count)++;
			/*
			 * Only blk-mq multiple hardware queues case checks the
			 * rq in the same queue, there should be only one such
			 * rq in a queue
			 **/
			if (same_queue_rq)
				*same_queue_rq = rq;
		}

		// 如果 bio 和当前 req 无法合并，继续遍历下一个 req
		if (rq->q != q || !blk_rq_merge_ok(rq, bio))
			continue;

		// 尝试合并
		el_ret = blk_try_merge(rq, bio);
		if (el_ret == ELEVATOR_BACK_MERGE) {
			ret = bio_attempt_back_merge(q, rq, bio);
			if (ret)
				break;
		} else if (el_ret == ELEVATOR_FRONT_MERGE) {
			ret = bio_attempt_front_merge(q, rq, bio);
			if (ret)
				break;
		}
	}
out:
	return ret;
}
```

当 bio 无法合入当前 request_queue 时，创建新的 request 请求，此时如果有 plug_list，则判断当前是否已经积攒足够的请求，如果是，则调用 blk_flush_plug_list 进行同步泄流（批量提交）。

```c
void blk_flush_plug_list(struct blk_plug *plug, bool from_schedule)
{
	struct request_queue *q;
	unsigned long flags;
	struct request *rq;
	LIST_HEAD(list);
	unsigned int depth;

	BUG_ON(plug->magic != PLUG_MAGIC);

	flush_plug_callbacks(plug, from_schedule);

	if (!list_empty(&plug->mq_list))
		blk_mq_flush_plug_list(plug, from_schedule);

	if (list_empty(&plug->list))
		return;

	list_splice_init(&plug->list, &list);

	/*
	 * 因为每个进程的 plug_list 可能包含多个设备的 request，所以对 request 进行排序。
	 * request_queue 相同（发往同一个设备）的 request 放在一起，按大小排序。
	 * 这样所有属于同一个设备的 request按照 IO 顺序组织起来。
	 */
	list_sort(NULL, &list, plug_rq_cmp);

	q = NULL;
	depth = 0;

	/*
	 * Save and disable interrupts here, to avoid doing it for every
	 * queue lock we have to take.
	 */
	local_irq_save(flags);
	/*
	 * 取出 plug_list 上已经排序的 request，判断它与当前 request_queue 是否相同。
	 * 如果相同，则将该 request 插入到调度队列。
	 * 如果不同，则进入下一轮 request_queue了，将当前 request_queue 进行 unplug。
	 */
	while (!list_empty(&list)) {
		rq = list_entry_rq(list.next);
		list_del_init(&rq->queuelist);
		BUG_ON(!rq->q);
		if (rq->q != q) {
			/*
			 * This drops the queue lock
			 */
			if (q)
				// 泄流
				queue_unplugged(q, depth, from_schedule);
			q = rq->q;
			depth = 0;
			spin_lock(q->queue_lock);
		}

		/*
		 * Short-circuit if @q is dead
		 */
		if (unlikely(blk_queue_dying(q))) {
			__blk_end_request_all(rq, -ENODEV);
			continue;
		}

		/*
		 * rq is already accounted, so use raw insert
		 */
		if (rq->cmd_flags & (REQ_FLUSH | REQ_FUA))
			__elv_add_request(q, rq, ELEVATOR_INSERT_FLUSH);
		else
			__elv_add_request(q, rq, ELEVATOR_INSERT_SORT_MERGE);

		depth++;
	}

	/*
	 * This drops the queue lock
	 */
	if (q)
		queue_unplugged(q, depth, from_schedule);

	local_irq_restore(flags);
}
```

第二个参数为 false 代表同步泄流，否则为异步泄流。

```c
static void queue_unplugged(struct request_queue *q, unsigned int depth,
			    bool from_schedule)
	__releases(q->queue_lock)
{
	trace_block_unplug(q, depth, !from_schedule);

	if (from_schedule)
		// 异步泄流
		blk_run_queue_async(q);
	else
		// 同步泄流
		__blk_run_queue(q);
	spin_unlock(q->queue_lock);
}

void __blk_run_queue(struct request_queue *q)
{
	if (unlikely(blk_queue_stopped(q)))
		return;

	__blk_run_queue_uncond(q);
}

inline void __blk_run_queue_uncond(struct request_queue *q)
{
	if (unlikely(blk_queue_dead(q)))
		return;

	/*
	 * Some request_fn implementations, e.g. scsi_request_fn(), unlock
	 * the queue lock internally. As a result multiple threads may be
	 * running such a request function concurrently. Keep track of the
	 * number of active request_fn invocations such that blk_drain_queue()
	 * can wait until all these request_fn calls have finished.
	 */
	q->request_fn_active++;
	q->request_fn(q);
	q->request_fn_active--;
}
```

可以看到同步泄流调用 `__blk_run_queue`，最后调用 `q->request_fn(q)`，即 SCSI 策略例程。



我们不能完全依赖同步 flush，很简单，因为如果在上层提交请求不足时可能会导致该 list 上的请求迟迟无法被调度。于是我们必须在一定的时候将这些 request 刷到设备的 request_queue 中。异步 flush 发生在进程切换时。

```c
void blk_run_queue_async(struct request_queue *q)
{
	if (likely(!blk_queue_stopped(q) && !blk_queue_dead(q)))
		mod_delayed_work(kblockd_workqueue, &q->delay_work, 0);
}
```

在 blk_alloc_queue_node 中会初始化 `INIT_DELAYED_WORK(&q->delay_work, blk_delay_work);`

blk_delay_work 调用了 `__blk_run_queue` 进行泄流。

```c
static void blk_delay_work(struct work_struct *work)
{
	struct request_queue *q;

	q = container_of(work, struct request_queue, delay_work.work);
	spin_lock_irq(q->queue_lock);
	__blk_run_queue(q);
	spin_unlock_irq(q->queue_lock);
}
```





---

# SCSI策略例程

scsi_request_fn 也只有一个参数，即指向请求队列描述符的指针。不过我们可以通过它找到对应的 SCSI 设备描述符，进而获得主机适配器描述符。SCSI 设备描述符保存在请求队列描述符的 queuedata 域，这是在 scsi_alloc_sdev 函数中，为 SCSI 设备分配请请求队列之后立即设置的。

我对 SCSI 也不是很熟悉，后面学习了 SCSI 子系统再来补充。

```c
static void scsi_request_fn(struct request_queue *q)
	__releases(q->queue_lock)
	__acquires(q->queue_lock)
{
	struct scsi_device *sdev = q->queuedata;
	struct Scsi_Host *shost;
	struct scsi_cmnd *cmd;
	struct request *req;

	/*
	 * To start with, we keep looping until the queue is empty, or until
	 * the host is no longer able to accept any more requests.
	 */
	shost = sdev->host;
	for (;;) {
		int rtn;
		/*
		 * 调用 blk_peek_request 函数获得下一个可排队的请求，我们这么早调用它，
		 * 是为了确保请求已经完全准备好，即使我们还不能接受它。
		 * 如果没有请求或者现在还不能向SCSI设备发送请求，则退出循环。
		 */
		req = blk_peek_request(q);
		if (!req)
			break;

		/*
		 * 如果设备已经离线，则输出错误消息，调用 scsi_kill_request 函数释放请求，
		 * 并以如此方式处理后面的所有请求。
		 */
		if (unlikely(!scsi_device_online(sdev))) {
			sdev_printk(KERN_ERR, sdev,
				    "rejecting I/O to offline device\n");
			scsi_kill_request(req, q);
			continue;
		}

		if (!scsi_dev_queue_ready(q, sdev))
			break;

		/*
		 * 如果队列不是使用 generic tag queueing，并且没有为请求启动 tagged 操作，
		 * 调用 blk_start_request 函数开始由驱动处理请求，
		 * 这个函数将请求从队列中取出，为它启动超时定时器。
		 */
		if (!(blk_queue_tagged(q) && !blk_queue_start_tag(q, req)))
			blk_start_request(req);

		spin_unlock_irq(q->queue_lock);

		/*
		 * 从块设备驱动层请求描述符的 special 域获得 SCSI 命令描述符，
		 * 这是在之前的 blk_peek_request 函数中，调用请求队列的 prep_rq_fn 回调函数准备好的。
		 * 它必然不为 NULL，否则只能说明出现了严重错误。
		 */
		cmd = req->special;
		if (unlikely(cmd == NULL)) {
			printk(KERN_CRIT "impossible request in %s.\n"
					 "please mail a stack trace to "
					 "linux-scsi@vger.kernel.org\n",
					 __func__);
			blk_dump_rq_flags(req, "foo");
			BUG();
		}

		/*
		 * We hit this when the driver is using a host wide
		 * tag map. For device level tag maps the queue_depth check
		 * in the device ready fn would prevent us from trying
		 * to allocate a tag. Since the map is a shared host resource
		 * we add the dev to the starved list so it eventually gets
		 * a run when a tag is freed.
		 */
		if (blk_queue_tagged(q) && !(req->cmd_flags & REQ_QUEUED)) {
			spin_lock_irq(shost->host_lock);
			if (list_empty(&sdev->starved_entry))
				list_add_tail(&sdev->starved_entry,
					      &shost->starved_list);
			spin_unlock_irq(shost->host_lock);
			goto not_ready;
		}

		/* 
		 * scsi_target_queue_ready 检查我们是否可以发送命令到这个目标节点，
		 * scsi_host_queue_ready 检查我们是否可以发送命令到这个主机适配器。
		 * 在队列没准备好的情况下，必须退出循环，将请求重新排入请求队列，
		 * 递减已经派发给 SCSI 设备（低层驱动）的命令数，
		 * 如果递减到 0，调用 blk_delay_queue 函数蓄流。
		 */
		if (!scsi_target_queue_ready(shost, sdev))
			goto not_ready;
		if (!scsi_host_queue_ready(q, shost, sdev))
			goto host_not_ready;
	
		if (sdev->simple_tags)
			cmd->flags |= SCMD_TAGGED;
		else
			cmd->flags &= ~SCMD_TAGGED;

		// 初始化错误处理参数，设置超时定时器。
		scsi_init_cmd_errh(cmd);

		/*
		 * 派发命令到低层驱动。如果该函数返回 0，表示派发成功，可继续循环；
		 * 否则说明出现错误，需要退出循环。
		 * 在退出之前，因为我们前面有放弃锁，在这里获得锁后，就需要再次检查是否需要蓄流。
		 */
		cmd->scsi_done = scsi_done;
		rtn = scsi_dispatch_cmd(cmd);
		if (rtn) {
			scsi_queue_insert(cmd, rtn);
			spin_lock_irq(q->queue_lock);
			goto out_delay;
		}
		spin_lock_irq(q->queue_lock);
	}

	return;

 host_not_ready:
	if (scsi_target(sdev)->can_queue > 0)
		atomic_dec(&scsi_target(sdev)->target_busy);
 not_ready:
	/*
	 * lock q, handle tag, requeue req, and decrement device_busy. We
	 * must return with queue_lock held.
	 *
	 * Decrementing device_busy without checking it is OK, as all such
	 * cases (host limits or settings) should run the queue at some
	 * later time.
	 */
	spin_lock_irq(q->queue_lock);
	blk_requeue_request(q, req);
	atomic_dec(&sdev->device_busy);
out_delay:
	if (!atomic_read(&sdev->device_busy) && !scsi_device_blocked(sdev))
		blk_delay_queue(q, SCSI_QUEUE_DELAY);
}
```

​	
