

# 概述

和数据恢复相关的主要有 4 个关键全局变量，其中 MRF 用于快速恢复。

```go
var (
    globalAllHealState *allHealState
    
    // The always present healing routine ready to heal objects
    globalBackgroundHealRoutine *healRoutine
    globalBackgroundHealState   *allHealState

    globalMRFState mrfState
)
```



---

# 初始化 HealState

前面我已经分析过 serverMain 是 minio server 初始化的主要函数。

```go
func serverMain(ctx *cli.Context) {
    ...
    initAllSubsystems()
    ...
    if globalIsErasure {
		initAutoHeal(GlobalContext, newObject)
		initHealMRF(GlobalContext, newObject)
	}
    ...
}
```

initAllSubsystems 负责子系统的初始化，其中就包括 HealState 的初始化，在 EC 模式下会初始化 globalAllHealState 和 globalBackgroundHealState 两个全局变量，他们都是 allHealState 类型。

```go
func initAllSubsystems() {
	if globalIsErasure {
		globalHealStateLK.Lock()
		// New global heal state
		globalAllHealState = newHealState(true)
		globalBackgroundHealState = newHealState(false)
		globalHealStateLK.Unlock()
	}
    ...
}
```

globalAllHealState 和 globalBackgroundHealState 都调用 newHealState 进行初始化，只是 cleanup 标志位不同。

```go
func newHealState(cleanup bool) *allHealState {
	hstate := &allHealState{
		healSeqMap:     make(map[string]*healSequence),
		healLocalDisks: map[Endpoint]struct{}{},
		healStatus:     make(map[string]healingTracker),
	}
	if cleanup {
		go hstate.periodicHealSeqsClean(GlobalContext)
	}
	return hstate
}
```

newHealState 返回 allHealState 结构，结构体中 **healSequence** 代表服务器上启动的每个修复序列的状态。

```go
type healSequence struct {
   // bucket, and object on which heal seq. was initiated
   bucket, object string
   // A channel of entities with heal result
   respCh chan healResult
   // Report healing progress
   reportProgress bool
   // time at which heal sequence was started
   startTime time.Time
   // time at which heal sequence has ended
   endTime time.Time
   // Heal client info
   clientToken, clientAddress string
   // was this heal sequence force started?
   forceStarted bool
   // heal settings applied to this heal sequence
   settings madmin.HealOpts
   // current accumulated status of the heal sequence
   currentStatus healSequenceStatus
   // channel signaled by background routine when traversal has completed
   traverseAndHealDoneCh chan error
   // canceler to cancel heal sequence.
   cancelCtx context.CancelFunc
   // the last result index sent to client
   lastSentResultIndex int64
   // Number of total items scanned against item type
   scannedItemsMap map[madmin.HealItemType]int64
   // Number of total items healed against item type
   healedItemsMap map[madmin.HealItemType]int64
   // Number of total items where healing failed against endpoint and drive state
   healFailedItemsMap map[string]int64
   // The time of the last scan/heal activity
   lastHealActivity time.Time
   // Holds the request-info for logging
   ctx context.Context
   // used to lock this structure as it is concurrently accessed
   mutex sync.RWMutex
}
```

healingTracker 用于在 heal 过程中持久保存 heal 信息。

```go
type healingTracker struct {
   disk StorageAPI `msg:"-"`

   ID         string
   PoolIndex  int
   SetIndex   int
   DiskIndex  int
   Path       string
   Endpoint   string
   Started    time.Time
   LastUpdate time.Time

   ObjectsTotalCount uint64
   ObjectsTotalSize  uint64

   ItemsHealed uint64
   ItemsFailed uint64

   BytesDone   uint64
   BytesFailed uint64

   // Last object scanned.
   Bucket string `json:"-"`
   Object string `json:"-"`

   // Numbers when current bucket started healing,
   // for resuming with correct numbers.
   ResumeItemsHealed uint64 `json:"-"`
   ResumeItemsFailed uint64 `json:"-"`
   ResumeBytesDone   uint64 `json:"-"`
   ResumeBytesFailed uint64 `json:"-"`

   // Filled on startup/restarts.
   QueuedBuckets []string

   // Filled during heal.
   HealedBuckets []string
   // Add future tracking capabilities
   // Be sure that they are included in toHealingDisk
}
```

periodicHealSeqsClean 用于定期清理已经完成的 healSequence（完成后保留 10 min 再删除）

```go
func (ahs *allHealState) periodicHealSeqsClean(ctx context.Context) {
   ...
   for {
      select {
      case <-periodicTimer.C:
         periodicTimer.Reset(time.Minute * 5)
         now := UTCNow()
         ahs.Lock()
         for path, h := range ahs.healSeqMap {
            // 已结束并且超过 10 min
            if h.hasEnded() && h.endTime.Add(keepHealSeqStateDuration).Before(now) {
               delete(ahs.healSeqMap, path)
            }
         }
         ahs.Unlock()
      case <-ctx.Done():
         // server could be restarting - need to exit immediately
         return
      }
   }
}
```



---

# initAutoHeal

上面初始化完成后 serverMain 继续运行，进入 initAutoHeal。

```go
func initAutoHeal(ctx context.Context, objAPI ObjectLayer) {
   z, ok := objAPI.(*erasureServerPools)
   if !ok {
      return
   }

   initBackgroundHealing(ctx, objAPI) // start quick background healing

   bgSeq := mustGetHealSequence(ctx)

   globalBackgroundHealState.pushHealLocalDisks(getLocalDisksToHeal()...)

   if drivesToHeal := globalBackgroundHealState.healDriveCount(); drivesToHeal > 0 {
      ...
      bgSeq.healDiskFormat()
      ...
   }

   go monitorLocalDisksAndHeal(ctx, z, bgSeq)
}
```

我们先讲述数据恢复的整体流程，然后再分析这个函数是怎么初始化的。

initBackgroundHealing 会初始化 **globalBackgroundHealRoutine**，它里面的 worker 负责实际的数据修复，待修复的对象由 **healTask** 表示，当 worker 的通道中接收到了任务就开始执行修复并且把结果写回 `healTask.respCh` ，这也是一个通道，它负责等待返回结果。

之前初始化的 HealState 负责创建 healSequence，并加入到自己的全局字典里（记录修复路径到 healSequence 的映射），然后开启协程 healSequenceStart 去处理修复。

经过层层调用，最后会使用 **queueHealTask** 把构造好的 healTask 发送出去并等待它完成，完成之后数据通过 channel 返回回来并填充到 healSequence 结果字段。

上面提到的 periodicHealSeqsClean 会定期清理 healSequence。

 ![](故障恢复/image-20220421144636037.png)



## initBackgroundHealing

```go
func initBackgroundHealing(ctx context.Context, objAPI ObjectLayer) {
   // Run the background healer
   globalBackgroundHealRoutine = newHealRoutine()
   for i := 0; i < globalBackgroundHealRoutine.workers; i++ {
      go globalBackgroundHealRoutine.AddWorker(ctx, objAPI)
   }

   globalBackgroundHealState.LaunchNewHealSequence(newBgHealSequence(), objAPI)
}
```

initBackgroundHealing 流程比较简单，下面我们分析它每个子函数里都在做什么。



### 后台协程结构体初始化

newHealRoutine 返回 healRoutine，它会初始化开启几个后台 routine 去处理数据恢复。

```go
func newHealRoutine() *healRoutine {
	workers := runtime.GOMAXPROCS(0) / 2
	if workers == 0 {
		workers = 4
	}
	return &healRoutine{
		tasks:   make(chan healTask),
		workers: workers,
	}
}
```

healTask 代表一个要修复的任务，其中有修复磁盘，修复 bucket，修复 object。

```go
// healTask represents what to heal along with options
//   path: '/' =>  Heal disk formats along with metadata
//   path: 'bucket/' or '/bucket/' => Heal bucket
//   path: 'bucket/object' => Heal object
type healTask struct {
   bucket    string
   object    string
   versionID string
   opts      madmin.HealOpts
   // Healing response will be sent here
   respCh chan healResult
}
```



### 开启后台协程

然后后台启动处理修复任务的 worker，这里可以看到根据修复任务的不同调用 ObjectLayer 的修复接口，并且把修复的结果通过通道传递出去。

```go
func (h *healRoutine) AddWorker(ctx context.Context, objAPI ObjectLayer) {
   for {
      select {
      case task, ok := <-h.tasks:
         if !ok {
            return
         }

         var res madmin.HealResultItem
         var err error
         // 根据 task 的字段进行相应的数据恢复
         switch task.bucket {
         case nopHeal:
            task.respCh <- healResult{err: errSkipFile}
            continue
         case SlashSeparator:
            res, err = healDiskFormat(ctx, objAPI, task.opts)
         default:
            if task.object == "" {
               res, err = objAPI.HealBucket(ctx, task.bucket, task.opts)
            } else {
               res, err = objAPI.HealObject(ctx, task.bucket, task.object, task.versionID, task.opts)
            }
         }

         // 处理结果发送到 respCh 通道
         task.respCh <- healResult{result: res, err: err}
      case <-ctx.Done():
         return
      }
   }
}
```



### 初始化发送一个 healSequence

newBgHealSequence 创建一个后台 healing sequence 操作，初始化流程这里的扫描方式是 HealNormalScan，表示不进行 Bitrot 检查。

```go
func newBgHealSequence() *healSequence {
   reqInfo := &logger.ReqInfo{API: "BackgroundHeal"}
   ctx, cancelCtx := context.WithCancel(logger.SetReqInfo(GlobalContext, reqInfo))

   hs := madmin.HealOpts{
      // Remove objects that do not have read-quorum
      Remove:   healDeleteDangling,
      ScanMode: globalHealConfig.ScanMode(),
   }

   return &healSequence{
      respCh:      make(chan healResult),
      startTime:   UTCNow(),
      clientToken: bgHealingUUID,
      // run-background heal with reserved bucket（minio）
      bucket:   minioReservedBucket,
      settings: hs,
      currentStatus: healSequenceStatus{
         Summary:      healNotStartedStatus,
         HealSettings: hs,
      },
      cancelCtx:          cancelCtx,
      ctx:                ctx,
      reportProgress:     false,
      scannedItemsMap:    make(map[madmin.HealItemType]int64),
      healedItemsMap:     make(map[madmin.HealItemType]int64),
      healFailedItemsMap: make(map[string]int64),
   }
}
```

LaunchNewHealSequence - 启动一个后台程序，根据 healSequence 参数执行修复。对于每个 healSequence，状态被存储在 `healSeqMap` 中，这是一个修复路径到 healSequence 的映射，它保存了关于 healSequence 的状态。

修复结果在服务器内存中持续保存，时间为 `keepHealSeqStateDuration`。periodicHealSeqsClean 在上述期限后清理修复结果。

```go
func (ahs *allHealState) LaunchNewHealSequence(h *healSequence, objAPI ObjectLayer) (
   respBytes []byte, apiErr APIError, errMsg string,
) {
   ...

   ahs.Lock()
   defer ahs.Unlock()

   // 检查要启动的 heal sequence 是否与任何现有的、正在运行的 heal sequence 相重叠
   hpath := pathJoin(h.bucket, h.object)
   for k, hSeq := range ahs.healSeqMap {
      if !hSeq.hasEnded() && (HasPrefix(k, hpath) || HasPrefix(hpath, k)) {
         ...
         return nil, errorCodes.ToAPIErr(ErrHealOverlappingPaths), errMsg
      }
   }

   // 添加到全局字典
   ahs.healSeqMap[hpath] = h
   // Launch top-level background heal go-routine
   go h.healSequenceStart(objAPI)

   ...

   b, err := json.Marshal(madmin.HealStartSuccess{
      ClientToken:   clientToken,
      ClientAddress: h.clientAddress,
      StartTime:     h.startTime,
   })
   ...
   return b, noError, ""
}
```

healSequenceStart - 这是顶层的后台修复程序。它启动另一个 go-routine，实际上是遍历磁盘上的数据，根据选定的设置检查和修复。这个程序

（1）监控遍历程序的完成情况，

（2）监听外部停止信号。

当任何一个事件发生时，它都会为 healSequence 设置完成状态。

```go
func (h *healSequence) healSequenceStart(objAPI ObjectLayer) {
   // Set status as running
   h.mutex.Lock()
   h.currentStatus.Summary = healRunningStatus
   h.currentStatus.StartTime = UTCNow()
   h.mutex.Unlock()

   go h.traverseAndHeal(objAPI)

   select {
   case err, ok := <-h.traverseAndHealDoneCh:
      if !ok {
         return
      }
      h.mutex.Lock()
      h.endTime = UTCNow()
      // Heal traversal is complete.
      if err == nil {
         // heal traversal succeeded.
         h.currentStatus.Summary = healFinishedStatus
      } else {
         // heal traversal had an error.
         h.currentStatus.Summary = healStoppedStatus
         h.currentStatus.FailureDetail = err.Error()
      }
      h.mutex.Unlock()
   case <-h.ctx.Done():
      ...
   }
}
```

traverseAndHeal - 遍历磁盘上的数据，并根据设置执行修复。在每个 "安全 "点，它还检查是否收到了外部退出信号，如果收到了，就退出。由于在收到外部退出信号时，遍历可能正在改变磁盘上的数据，这个例程不能立即退出，必须等到达到一个安全点，比如在扫描两个对象之间。

```go
func (h *healSequence) traverseAndHeal(objAPI ObjectLayer) {
   bucketsOnly := false // Heals buckets and objects also.
   h.traverseAndHealDoneCh <- h.healItems(objAPI, bucketsOnly)
   close(h.traverseAndHealDoneCh)
}

func (h *healSequence) healItems(objAPI ObjectLayer, bucketsOnly bool) error {
	if err := h.healDiskMeta(objAPI); err != nil {
		return err
	}

	// Heal buckets and objects
	return h.healBuckets(objAPI, bucketsOnly)
}
```

这里的修复最终会调用 queueHealTask 生成 healTask 发送给 globalBackgroundHealRoutine 去处理，处理完毕后会把结果写入 task 的 respCh 字段，queueHealTask 从 respCh 收到回复之后，进行一些处理。

```go
func (h *healSequence) queueHealTask(source healSource, healType madmin.HealItemType) error {
	// Send heal request
	task := healTask{
		bucket:    source.bucket,
		object:    source.object,
		versionID: source.versionID,
		opts:      h.settings,
		respCh:    h.respCh,
	}
	...

	select {
	case res := <-h.respCh:
		if !h.reportProgress {
			if errors.Is(res.err, errSkipFile) { // this is only sent usually by nopHeal
				return nil
			}

			h.mutex.Lock()
			defer h.mutex.Unlock()
			if res.err != nil {
				for _, d := range res.result.After.Drives {
					h.healFailedItemsMap[d.Endpoint+","+d.State]++
				}
			} else {
				// Only object type reported for successful healing
				h.healedItemsMap[res.result.Type]++
			}

			// Report caller of any failure
			return res.err
		}
		...
	}
}
```







## 本地磁盘修复

分析完 initBackgroundHealing 我们回到 initAutoHeal，mustGetHealSequence 会阻塞直到有一个 heal Sequence，这里至少有一个在 initBackgroundHealing 中构造的数据检查。

getLocalDisksToHeal 收集本地需要修复的磁盘，pushHealLocalDisks 将其保存到 globalBackgroundHealState.healLocalDisks。

```go
func getLocalDisksToHeal() (disksToHeal Endpoints) {
   for _, disk := range globalLocalDrives {
      _, err := disk.GetDiskID()
      if errors.Is(err, errUnformattedDisk) {
         disksToHeal = append(disksToHeal, disk.Endpoint())
         continue
      }
      if disk.Healing() != nil {
         disksToHeal = append(disksToHeal, disk.Endpoint())
      }
   }
   if len(disksToHeal) == globalEndpoints.NEndpoints() {
      // When all disks == all command line endpoints
      // this is a fresh setup, no need to trigger healing.
      return Endpoints{}
   }
   return disksToHeal
}
```

globalBackgroundHealState.healDriveCount() 读取上述结果，如果有需要修复的磁盘，尽早修复任何磁盘 format 和 metadata。从修复 disk format 开始，也是简单调用 queueHealTask 然后让后台程序处理即可。

```go
func (h *healSequence) healDiskFormat() error {
   if h.isQuitting() {
      return errHealStopSignalled
   }

   return h.queueHealTask(healSource{bucket: SlashSeparator}, madmin.HealItemMetadata)
}
```



## monitorLocalDisksAndHeal

monitorLocalDisksAndHeal - 确保检测到的新磁盘被修复。
1. 只有相关的擦除集会被列出并修复。
2. 只有托管磁盘的节点负责执行修复。

```go
func monitorLocalDisksAndHeal(ctx context.Context, z *erasureServerPools, bgSeq *healSequence) {
   // Perform automatic disk healing when a disk is replaced locally.
   diskCheckTimer := time.NewTimer(defaultMonitorNewDiskInterval)
   defer diskCheckTimer.Stop()

   for {
      select {
      case <-ctx.Done():
         return
      case <-diskCheckTimer.C:
         // Reset to next interval.
         diskCheckTimer.Reset(defaultMonitorNewDiskInterval)

         var erasureSetInPoolDisksToHeal []map[int][]StorageAPI

         healDisks := globalBackgroundHealState.getHealLocalDiskEndpoints()
         // 如果有要修复的磁盘
         if len(healDisks) > 0 {
            // Reformat disks
            bgSeq.queueHealTask(healSource{bucket: SlashSeparator}, madmin.HealItemMetadata)

            // Ensure that reformatting disks is finished
            bgSeq.queueHealTask(healSource{bucket: nopHeal}, madmin.HealItemMetadata)

            // 为 rasureSetInPoolDisksToHeal 分配空间
            erasureSetInPoolDisksToHeal = make([]map[int][]StorageAPI, len(z.serverPools))
            for i := range z.serverPools {
               erasureSetInPoolDisksToHeal[i] = map[int][]StorageAPI{}
            }
         }

         ...

         // 只有在发现新的磁盘时才会修复
         for _, endpoint := range healDisks {
            disk, format, err := connectEndpoint(endpoint)
            ...
            poolIdx := globalEndpoints.GetLocalPoolIdx(disk.Endpoint())
            ...

            // Calculate the set index where the current endpoint belongs
            z.serverPools[poolIdx].erasureDisksMu.RLock()
            setIndex, _, err := findDiskIndex(z.serverPools[poolIdx].format, format)
            z.serverPools[poolIdx].erasureDisksMu.RUnlock()
            ...

            erasureSetInPoolDisksToHeal[poolIdx][setIndex] = append(erasureSetInPoolDisksToHeal[poolIdx][setIndex], disk)
         }

         buckets, _ := z.ListBuckets(ctx)

         // 桶的数据分散在多个区域，确保修复所有桶的元数据配置。
         buckets = append(buckets, BucketInfo{
            Name: pathJoin(minioMetaBucket, minioConfigPrefix),
         }, BucketInfo{
            Name: pathJoin(minioMetaBucket, bucketMetaPrefix),
         })

         // 排序，先修复最新的 bucket
         sort.Slice(buckets, func(i, j int) bool {
            a, b := strings.HasPrefix(buckets[i].Name, minioMetaBucket), strings.HasPrefix(buckets[j].Name, minioMetaBucket)
            if a != b {
               return a
            }
            return buckets[i].Created.After(buckets[j].Created)
         })

         // TODO(klauspost): This will block until all heals are done,
         // in the future this should be able to start healing other sets at once.
         var wg sync.WaitGroup
         for i, setMap := range erasureSetInPoolDisksToHeal {
            i := i
            for setIndex, disks := range setMap {
               if len(disks) == 0 {
                  continue
               }
               wg.Add(1)
               // 执行修复
               go func(setIndex int, disks []StorageAPI) {
                  defer wg.Done()
                  for _, disk := range disks {
                     // So someone changed the drives underneath, healing tracker missing.
                     tracker, err := loadHealingTracker(ctx, disk)
                     if err != nil {
                        tracker = newHealingTracker(disk)
                     }

                     // Load bucket totals
                     cache := dataUsageCache{}
                     if err := cache.load(ctx, z.serverPools[i].sets[setIndex], dataUsageCacheName); err == nil {
                        dataUsageInfo := cache.dui(dataUsageRoot, nil)
                        tracker.ObjectsTotalCount = dataUsageInfo.ObjectsTotalCount
                        tracker.ObjectsTotalSize = dataUsageInfo.ObjectsTotalSize
                     }

                     tracker.PoolIndex, tracker.SetIndex, tracker.DiskIndex = disk.GetDiskLoc()
                     tracker.setQueuedBuckets(buckets)
                     if err := tracker.save(ctx); err != nil {
                        logger.LogIf(ctx, err)
                        // Unable to write healing tracker, permission denied or some
                        // other unexpected error occurred. Proceed to look for new
                        // disks to be healed again, we cannot proceed further.
                        return
                     }

                     err = z.serverPools[i].sets[setIndex].healErasureSet(ctx, tracker.QueuedBuckets, tracker)
                     if err != nil {
                        logger.LogIf(ctx, err)
                        continue
                     }

                     // Only upon success pop the healed disk.
                     globalBackgroundHealState.popHealLocalDisks(disk.Endpoint())
                  }
               }(setIndex, disks)
            }
         }
         wg.Wait()
      }
   }
}
```

那么磁盘监控是在哪里初始化的呢？

在初始化 EC 对象层时会初始化 set，newErasureSets 会开启 goroutine monitorAndConnectEndpoints。

monitorAndConnectEndpoints 是一个监测循环，通过重新连接来跟踪断开的端点，并确保将它们放置在设定的拓扑结构中的正确位置。

```go
func (s *erasureSets) monitorAndConnectEndpoints(ctx context.Context, monitorInterval time.Duration) {
   r := rand.New(rand.NewSource(time.Now().UnixNano()))

   time.Sleep(time.Duration(r.Float64() * float64(time.Second)))

   // Pre-emptively connect the disks if possible.
   s.connectDisks()

   monitor := time.NewTimer(monitorInterval)
   defer monitor.Stop()

   for {
      select {
      case <-ctx.Done():
         return
      case <-monitor.C:
         // Reset the timer once fired for required interval.
         monitor.Reset(monitorInterval)

         if serverDebugLog {
            console.Debugln("running disk monitoring")
         }

         s.connectDisks()
      }
   }
}
```

connectDisks 试图连接所有的端点，加载格式并将磁盘重新排列到适当的位置。检测到需要修复时，调用 pushHealLocalDisks 将磁盘信息传入到 healState。我们在后面再详细分析这个函数。







---

# initHealMRF

MRF 用于快速恢复。

```go
var globalMRFState mrfState

type mrfState struct {
	ready int32 // ref: https://golang.org/pkg/sync/atomic/#pkg-note-BUG
	_     int32 // For 64 bits alignment

	ctx       context.Context
	objectAPI ObjectLayer

	mu                sync.Mutex
    // partialOperation 是指成功上传/删除一个对象，但没有在所有磁盘中写入（部分磁盘故障的时候）。
	opCh              chan partialOperation
	pendingOps        map[partialOperation]setInfo
	setReconnectEvent chan setInfo

     // 已修复的数据计数
	itemsHealed  uint64
	bytesHealed  uint64
    // 字典里的数据计数
	pendingItems uint64
	pendingBytes uint64

	triggeredAt time.Time
}
```

partialOperation 是指成功上传/删除一个对象，但没有在所有磁盘中写入（部分磁盘故障的时候）

```go
type partialOperation struct {
   bucket    string
   object    string
   versionID string
   size      int64
   setIndex  int
   poolIndex int
}

type setInfo struct {
   index, pool int
}
```



init 除了做了些简单的初始化，就是开启了两个 goroutine。

```go
func initHealMRF(ctx context.Context, obj ObjectLayer) {
   globalMRFState.init(ctx, obj)
}

func (m *mrfState) init(ctx context.Context, objAPI ObjectLayer) {
	m.mu.Lock()
	defer m.mu.Unlock()

	m.ctx = ctx
	m.objectAPI = objAPI
	m.opCh = make(chan partialOperation, mrfOpsQueueSize)
	m.pendingOps = make(map[partialOperation]setInfo)
	m.setReconnectEvent = make(chan setInfo)

	go globalMRFState.maintainMRFList()
	go globalMRFState.healRoutine()

	atomic.StoreInt32(&m.ready, 1)
}
```

maintainMRFList 从所有底层 er.sets 收集成功的部分上传列表，并把它们放在一个全局字典（pendingOps）中，该字典不应该有超过 10000 个条目，这就表示在磁盘失联的这段时间，当 `m.pendingOps` 条目超过 10000 时，新的部分写入的对象没法通过 MRF 快速修复，这些对象只有被访问或者被 DataScanner 的时候才会被发现写入不完全。

```go
func (m *mrfState) maintainMRFList() {
   for fOp := range m.opCh {
      m.mu.Lock()
      // 超过 10000 就暂时不先塞进来了
      if len(m.pendingOps) > mrfOpsQueueSize {
         m.mu.Unlock()
         continue
      }

      // 加入全局字典
      m.pendingOps[fOp] = setInfo{index: fOp.setIndex, pool: fOp.poolIndex}
      m.pendingItems++
      if fOp.size > 0 {
         m.pendingBytes += uint64(fOp.size)
      }

      m.mu.Unlock()
   }
}
```

healRoutine 监听新磁盘的重新连接事件，并为属于相应擦除集的排队对象发出修复请求。

```go
func (m *mrfState) healRoutine() {
   ...

   for {
      idler.Reset(mrfInfoResetInterval)
      select {
      case <-m.ctx.Done():
         return
      case <-idler.C:
         m.resetMRFInfoIfNoPendingOps()
      case setInfo := <-m.setReconnectEvent:
         // 获取连接的磁盘所属的 er.set 相关对象的列表。
         var mrfOperations []partialOperation
         m.mu.Lock()
         for k, v := range m.pendingOps {
            if v == setInfo {
               mrfOperations = append(mrfOperations, k)
            }
         }
         m.mu.Unlock()
         ...

         // Heal objects
         for _, u := range mrfOperations {
            m.objectAPI.HealObject(m.ctx, u.bucket, u.object, u.versionID, mrfHealingOpts)
            ...

            m.mu.Lock()
            delete(m.pendingOps, u)
            m.mu.Unlock()
         }

         waitForLowHTTPReq()
      }
   }
}
```

看到了处理流程我们大概也能猜到，在写入或删除对象时，会生成一个 partialOperation，然后把它塞到 `globalMRFState.opCh` 通道中，addPartial 就是实现这个功能的函数。

```go
func (er erasureObjects) addPartial(bucket, object, versionID string, size int64) {
   globalMRFState.addPartialOp(partialOperation{
      bucket:    bucket,
      object:    object,
      versionID: versionID,
      size:      size,
      setIndex:  er.setIndex,
      poolIndex: er.poolIndex,
   })
}

func (m *mrfState) addPartialOp(op partialOperation) {
	if !m.initialized() {
		return
	}

	select {
	case m.opCh <- op:
	default:
	}
}
```

而快速恢复触发的条件是 set 恢复，我们可以在 `func (s *erasureSets) connectDisks()` 方法中找到，当一个 set 恢复后，我们向 `globalMRFState.setReconnectEvent` 发送一个 setInfo。

```go
func (m *mrfState) newSetReconnected(pool, set int) {
   if !m.initialized() {
      return
   }

   idler := time.NewTimer(100 * time.Millisecond)
   defer idler.Stop()

   select {
   case m.setReconnectEvent <- setInfo{index: set, pool: pool}:
   case <-idler.C:
   }
}
```





---

# connectDisks

这个函数负责监控磁盘状态，流程见注释。

```go
func (s *erasureSets) connectDisks() {
   defer func() {
      s.lastConnectDisksOpTime = time.Now()
   }()

   var wg sync.WaitGroup
   // 获取 set 组 drives 的信息
   diskMap := s.getDiskMap()
   setsJustConnected := make([]bool, s.setCount)
   for _, endpoint := range s.endpoints.Endpoints {
      // diskMap 里都是好的磁盘
      cdisk := diskMap[endpoint]
      // 检查在线的磁盘最近是否掉线
      if cdisk != nil && cdisk.IsOnline() {
         if s.lastConnectDisksOpTime.IsZero() {
            continue
         }

         // 一个在线的磁盘也可能最近掉线过，这里通过磁盘最后的连接时间来比较
         _, setIndex, _ := cdisk.GetDiskLoc()
         if setIndex != -1 {
            // 最近掉线的磁盘通过 MRF 恢复，不过这里 LastConn 好像是个空接口？
            setsJustConnected[setIndex] = cdisk.LastConn().After(s.lastConnectDisksOpTime)
            continue
         }
      }

      wg.Add(1)
      go func(endpoint Endpoint) {
         defer wg.Done()
         disk, format, err := connectEndpoint(endpoint)
         if err != nil {
            // 检测到磁盘未 format，通过 healState 修复
            if endpoint.IsLocal && errors.Is(err, errUnformattedDisk) {
               globalBackgroundHealState.pushHealLocalDisks(endpoint)
            } else {
               printEndpointError(endpoint, err, true)
            }
            return
         }
         ...
   }

   wg.Wait()

   // 最近重连的磁盘所属的 set 组通过 MRF 快速恢复
   go func() {
      for setIndex, justConnected := range setsJustConnected {
         if !justConnected {
            continue
         }
         globalMRFState.newSetReconnected(s.poolIndex, setIndex)
      }
   }()
}
```

getDiskMap 获取 set 里正常的磁盘信息。此时会检查磁盘是否在 set 检查之后掉线过，判断依据就是 LastConn 接口方法的值和 lastConnectDisksOpTime 比较，但是这里 LastConn 是个空接口，也就是永远不会触发？？？

```go
func (s *erasureSets) getDiskMap() map[Endpoint]StorageAPI {
   diskMap := make(map[Endpoint]StorageAPI)

   s.erasureDisksMu.RLock()
   defer s.erasureDisksMu.RUnlock()

   for i := 0; i < s.setCount; i++ {
      for j := 0; j < s.setDriveCount; j++ {
         disk := s.erasureDisks[i][j]
         if disk == OfflineDisk {
            continue
         }
         if !disk.IsOnline() {
            continue
         }
         diskMap[disk.Endpoint()] = disk
      }
   }
   return diskMap
}
```

其余的情况就是得在 GetDiskLoc 返回的值为 -1，也就是磁盘的索引找不到，connectEndpoint 会去获取磁盘的 format，如果是新替换的盘，返回结果为 errUnformattedDisk，那么需要去恢复磁盘数据。

这里让人比较疑惑的是什么时候会找不到 setIndex，且在找不到之后后续还会尝试找回来并判断是否正确，一切正确后才会重新加入 set。

这么看来，如果是临时掉线或者网络故障的盘，不会去设置 setsJustConnected 来触发 MRF（总感觉我理解错了，可是测试好像也是这样……）。



抛开疑惑，我们还是来看下怎么检查磁盘恢复的。首先如果一个磁盘故障被替换了，那么肯定获取不到 format，此时 connectEndpoint 就会返回 errUnformattedDisk，磁盘数据进行恢复。但是可能恢复到一半，所以后面的代码都是在处理怎么继续恢复。

```go
func(endpoint Endpoint) {
	defer wg.Done()
	disk, format, err := connectEndpoint(endpoint)
	if err != nil {
         // 检测到磁盘未 format，通过 healState 修复
		if endpoint.IsLocal && errors.Is(err, errUnformattedDisk) {
			globalBackgroundHealState.pushHealLocalDisks(endpoint)
		} else {
			printEndpointError(endpoint, err, true)
		}
		return
	}
	if disk.IsLocal() && disk.Healing() != nil {
         // 磁盘未恢复完全就被中断了，这里继续执行恢复
		globalBackgroundHealState.pushHealLocalDisks(disk.Endpoint())
	}
	s.erasureDisksMu.RLock()
	setIndex, diskIndex, err := findDiskIndex(s.format, format)
	s.erasureDisksMu.RUnlock()
	if err != nil {
		printEndpointError(endpoint, err, false)
		disk.Close()
		return
	}

	s.erasureDisksMu.Lock()
	if currentDisk := s.erasureDisks[setIndex][diskIndex]; currentDisk != nil {
		if !reflect.DeepEqual(currentDisk.Endpoint(), disk.Endpoint()) {
             // 磁盘信息不正确
			err = fmt.Errorf("Detected unexpected disk ordering refusing to use the disk: expecting %s, found %s, refusing to use the disk",
				currentDisk.Endpoint(), disk.Endpoint())
			printEndpointError(endpoint, err, false)
			disk.Close()
			s.erasureDisksMu.Unlock()
			return
		}
		s.erasureDisks[setIndex][diskIndex].Close()
	}
	if disk.IsLocal() {
		disk.SetDiskID(format.Erasure.This)
		s.erasureDisks[setIndex][diskIndex] = disk
	} else {
		// Enable healthcheck disk for remote endpoint.
		disk, err = newStorageAPI(endpoint)
		if err != nil {
			printEndpointError(endpoint, err, false)
			s.erasureDisksMu.Unlock()
			return
		}
		disk.SetDiskID(format.Erasure.This)
		s.erasureDisks[setIndex][diskIndex] = disk
	}
	disk.SetDiskLoc(s.poolIndex, setIndex, diskIndex)
    // 磁盘刚 online，我们要快速恢复 MRF 队列里的对象
	setsJustConnected[setIndex] = true
	s.erasureDisksMu.Unlock()
}(endpoint)
```





---

# 数据恢复实例

上面讲的有些枯燥，可能有些混乱，下面结合实例把流程理清一下，假设 set 中某个盘坏了，此时替换了一块新盘。

首先 monitorAndConnectEndpoints 会定期调用 connectDisks 去检查磁盘状态，因为磁盘是新的，所以上面没有 format 信息，在 connectDisks 中检查到 errUnformattedDisk 错误后会调用 globalBackgroundHealState.pushHealLocalDisks(endpoint) 将信息保存到了 globalBackgroundHealState.healLocalDisks 中。

monitorLocalDisksAndHeal 定期检查是否有故障的磁盘要恢复，它会从 globalBackgroundHealState.healLocalDisks 去获取要恢复的磁盘列表，如果存在要恢复的磁盘，那么就要执行恢复操作。

首先是要恢复 format 信息，这里会使用 queueHealTask 把构造好的 healTask 发送出去并等待它完成。

```go
if len(healDisks) > 0 {
	// Reformat disks
	bgSeq.queueHealTask(healSource{bucket: SlashSeparator}, madmin.HealItemMetadata)

	// Ensure that reformatting disks is finished
	bgSeq.queueHealTask(healSource{bucket: nopHeal}, madmin.HealItemMetadata)

	logger.Info(fmt.Sprintf("Found drives to heal %d, proceeding to heal - 'mc admin heal alias/ --verbose' to check the status.",
		len(healDisks)))

	erasureSetInPoolDisksToHeal = make([]map[int][]StorageAPI, len(z.serverPools))
	for i := range z.serverPools {
		erasureSetInPoolDisksToHeal[i] = map[int][]StorageAPI{}
	}
}
```

后台 Worker 恢复处理程序会根据参数来决定具体的操作，这里很明显是会调用 healDiskFormat 去恢复 format。

```go
switch task.bucket {
	case nopHeal:
		task.respCh <- healResult{err: errSkipFile}
		continue
	case SlashSeparator:
		res, err = healDiskFormat(ctx, objAPI, task.opts)
	default:
		if task.object == "" {
			res, err = objAPI.HealBucket(ctx, task.bucket, task.opts)
		} else {
			res, err = objAPI.HealObject(ctx, task.bucket, task.object, task.versionID, task.opts)
		}
	}
```

erasureSetInPoolDisksToHeal 用于记录要修复的磁盘的 StorageAPI，这个没什么好讲的。

```go
for _, endpoint := range healDisks {
	disk, format, err := connectEndpoint(endpoint)
	if err != nil {
		printEndpointError(endpoint, err, true)
		continue
	}

    poolIdx := globalEndpoints.GetLocalPoolIdx(disk.Endpoint())
	if poolIdx < 0 {
		continue
	}

    // Calculate the set index where the current endpoint belongs
	z.serverPools[poolIdx].erasureDisksMu.RLock()
	// Protect reading reference format.
	setIndex, _, err := findDiskIndex(z.serverPools[poolIdx].format, format)
	z.serverPools[poolIdx].erasureDisksMu.RUnlock()
	if err != nil {
		printEndpointError(endpoint, err, false)
		continue
	}

	erasureSetInPoolDisksToHeal[poolIdx][setIndex] = append(erasureSetInPoolDisksToHeal[poolIdx][setIndex], disk)
}
```

接下来的工作可想而知，是要恢复每个盘上的 bucket 及其对象。具体执行 goroutine 如下，其中 healErasureSet 负责主要数据的恢复。

```go
go func(setIndex int, disks []StorageAPI) {
    defer wg.Done()
    for _, disk := range disks {
        // So someone changed the drives underneath, healing tracker missing.
        tracker, err := loadHealingTracker(ctx, disk)
        ...

        // Load bucket totals
        cache := dataUsageCache{}
        if err := cache.load(ctx, z.serverPools[i].sets[setIndex], dataUsageCacheName); err == nil {
            dataUsageInfo := cache.dui(dataUsageRoot, nil)
            tracker.ObjectsTotalCount = dataUsageInfo.ObjectsTotalCount
            tracker.ObjectsTotalSize = dataUsageInfo.ObjectsTotalSize
        }

        tracker.PoolIndex, tracker.SetIndex, tracker.DiskIndex = disk.GetDiskLoc()
        tracker.setQueuedBuckets(buckets)
        if err := tracker.save(ctx); err != nil {
            logger.LogIf(ctx, err)
            // Unable to write healing tracker, permission denied or some
            // other unexpected error occurred. Proceed to look for new
            // disks to be healed again, we cannot proceed further.
            return
        }

        err = z.serverPools[i].sets[setIndex].healErasureSet(ctx, tracker.QueuedBuckets, tracker)
        ...

        // Only upon success pop the healed disk.
        globalBackgroundHealState.popHealLocalDisks(disk.Endpoint())
    }
}(setIndex, disks)
```







