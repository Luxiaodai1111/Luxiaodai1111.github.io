在分析 minio server 启动流程时，我们介绍了 HTTP API 的注册，本文主要分析 EC 模式下对象的 PUT / GET 操作。

---

# PUT 流程

## 入口检查

首先我们找到 API 入口，在初始化时，api 会被初始化成对应的对象层。

```go
// PutObject
router.Methods(http.MethodPut).Path("/{object:.+}").HandlerFunc(
   collectAPIStats("putobject", maxClients(gz(httpTraceHdrs(api.PutObjectHandler)))))
```

PutObjectHandler 流程如下：

1.  首先获取对象层 API，前面我们介绍过对象层目前有两种，一种是 FSObjects，这种模式后端只有一个存储点，另一种是 erasureServerPools，也是本文重点要介绍的 EC 模式
2.  如果 HTTP 头信息表明要求某种形式的服务器端加密，IsRequested 返回 true 和 SSE 类型。如果没有 SSE 头信息，那么 IsRequested 返回 false，并且没有 Type
3.  从 HTTP 请求中提取要操作的 bucket 和 object 以及各种信息
4.  检查配额、加密、压缩、桶复制状态、WORM 等等，并做相应的处理
5.  调用对象层 PutObject 接口去实际创建对象
6.  返回结果

```go
func (api objectAPIHandlers) PutObjectHandler(w http.ResponseWriter, r *http.Request) {
   ctx := newContext(r, w, "PutObject")
   defer logger.AuditLog(ctx, w, r, mustGetClaimsFromToken(r))

   // 获取对象层
   objectAPI := api.ObjectAPI()
   ...

   // 判断请求是否要求加密
   if _, ok := crypto.IsRequested(r.Header); ok {
      ...
   }

   // 提取要操作的 bucket 和 object
   vars := mux.Vars(r)
   bucket := vars["bucket"]
   object, err := unescapePath(vars["object"])
   ...

   // 检查 storage class，如果指定了只能为 RRS|STANDARD
   if sc := r.Header.Get(xhttp.AmzStorageClass); sc != "" {
      if !storageclass.IsValid(sc) {
         writeErrorResponse(ctx, w, errorCodes.ToAPIErr(ErrInvalidStorageClass), r.URL)
         return
      }
   }

   // FromContentMD5 负责解码并将 Content-MD5 值作为 ETag 返回。如果没有设置 Content-MD5 头，则返回一个空的 ETag
   clientETag, err := etag.FromContentMD5(r.Header)
   ...

   // 如果 Content-Length 未知/缺失，则拒绝该请求
   size := r.ContentLength
   rAuthType := getRequestAuthType(r)
   if rAuthType == authTypeStreamingSigned {
      // authTypeStreamingSigned 要重新计算 size
      if sizeStr, ok := r.Header[xhttp.AmzDecodedContentLength]; ok {
         size, err = strconv.ParseInt(sizeStr[0], 10, 64) 
      }
   }

   // 单次操作最大支持 5T
   if isMaxObjectSize(size) {
      writeErrorResponse(ctx, w, errorCodes.ToAPIErr(ErrEntityTooLarge), r.URL)
      return
   }

   // extractMetadata 从 HTTP 头和 HTTP 查询字符串中提取元数据。
   metadata, err := extractMetadata(ctx, r)

   if objTags := r.Header.Get(xhttp.AmzObjectTagging); objTags != "" {
      ...
      metadata[xhttp.AmzObjectTagging] = objTags
   }

   var (
      md5hex              = clientETag.String()
      sha256hex           = ""
      reader    io.Reader = r.Body		// 用户数据
      s3Err     APIErrorCode
      putObject = objectAPI.PutObject	// 对象层 PutObject 实现 API
   )

   // 检查资源上是否允许 PUT 操作，这个调用验证了桶策略和 IAM 策略，支持多用户检查等。
   if s3Err = isPutActionAllowed(ctx, rAuthType, bucket, object, r, iampolicy.PutObjectAction); s3Err != ErrNone {
      writeErrorResponse(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)
      return
   }

   switch rAuthType {
   case authTypeStreamingSigned:
      /*
       * newSignV4ChunkedReader 返回一个 s3ChunkedReader，在返回之前将从 r 读取的数据转换成 HTTP "chunked "格式。
       * s3ChunkedReader 在读完最后一个 0 长度的分块时返回 io.EOF。
       * 一般的应用程序不需要 NewChunkedReader。http 包在读取响应体时自动解码 chunking。
       */
      reader, s3Err = newSignV4ChunkedReader(r)
      ...
   }

   // 检查配额
   if err := enforceBucketQuotaHard(ctx, bucket, size); err != nil {
      writeErrorResponse(ctx, w, toAPIError(ctx, err), r.URL)
      return
   }
   // 桶复制相关
   if r.Header.Get(xhttp.AmzBucketReplicationStatus) == replication.Replica.String() {
      ...
   }

   // 检查是否启用了桶的加密功能
   sseConfig, _ := globalBucketSSEConfigSys.Get(bucket)
   sseConfig.Apply(r.Header, sse.ApplyOptions{
      AutoEncrypt: globalAutoEncryption,
      Passthrough: globalIsGateway && globalGatewayName == S3BackendGateway,
   })

   actualSize := size
   /*
    * 对加密启用的请求禁用压缩。同时使用压缩和加密可以为侧信道攻击提供空间。
    * 通过 extensions/content-types 消除不可压缩的对象。
    */
   if objectAPI.IsCompressionSupported() && isCompressible(r.Header, object) && size > 0 {
      ...
   }

   hashReader, err := hash.NewReader(reader, size, md5hex, sha256hex, actualSize)
   rawReader := hashReader
   pReader := NewPutObjReader(rawReader)

   // get gateway encryption options
   var opts ObjectOptions
   opts, err = putOpts(ctx, r, bucket, object, metadata)

   if api.CacheAPI() != nil {
      putObject = api.CacheAPI().PutObject
   }

   retPerms := isPutActionAllowed(ctx, getRequestAuthType(r), bucket, object, r, iampolicy.PutObjectRetentionAction)
   holdPerms := isPutActionAllowed(ctx, getRequestAuthType(r), bucket, object, r, iampolicy.PutObjectLegalHoldAction)

   getObjectInfo := objectAPI.GetObjectInfo
   if api.CacheAPI() != nil {
      getObjectInfo = api.CacheAPI().GetObjectInfo
   }

   // 对带有 WORM 头的请求执行对象保留策略和合法保留策略
   retentionMode, retentionDate, legalHold, s3Err := checkPutObjectLockAllowed(ctx, r, bucket, object, getObjectInfo, retPerms, holdPerms)
   ...

   // 确保元数据不包含敏感信息
   crypto.RemoveSensitiveEntries(metadata)

   // 使用桶的名称来初始化版本信息
   os := newObjSweeper(bucket, object).WithVersioning(opts.Versioned, opts.VersionSuspended)
   ...

   // 创建对象
   objInfo, err := putObject(ctx, bucket, object, pReader, opts)

   ...

   setPutObjHeaders(w, objInfo, false)
   writeSuccessResponseHeadersOnly(w)

   // 通知对象创建事件
   sendEvent(eventArgs{
      EventName:    event.ObjectCreatedPut,
      BucketName:   bucket,
      Object:       objInfo,
      ReqParams:    extractReqParams(r),
      RespElements: extractRespElements(w),
      UserAgent:    r.UserAgent(),
      Host:         handlers.GetSourceIP(r),
   })

   // Remove the transitioned object whose object version is being overwritten.
   if !globalTierConfigMgr.Empty() {
      // Schedule object for immediate transition if eligible.
      enqueueTransitionImmediate(objInfo)
      logger.LogIf(ctx, os.Sweep())
   }
}
```



## 选择存储池

EC 模式的 PutObject 接口主要是一个选择存储池的过程。

1.  checkPutObjectArgs 检查桶是否存在和对象名是否合法。
2.  encodeDirObject 负责对目录对象编码。在 MinIO 上，一个目录对象被存储为一个普通对象，后缀为 `__XLDIR__`。例如。`prefix/` 被存储为 `prefix__XLDIR__`。
3.  如果只有一个池，就直接调用池的 PutObject 接口，否则要选择存储池进行操作。
4.  getPoolIdxNoLock 负责选取存储池：
    -   首先 getPoolIdxExistingNoLock 检查对象是否存在于某个存储池内，如果找到的话返回包含一个对象的（第一个）发现的对象池索引。如果对象存在，但最新版本是一个删除标记，仍然返回包含它的索引。
    -   如果找不到对象，则调用 getAvailablePoolIdx 选择存储池，算法首先计算所有的存储池剩余容量，然后生成随机数对总容量取余，落在哪个空间就存在哪，这样对象会倾向于存储到剩余空间更大的池里。比如两个池，一个剩余空间为 100，一个剩余空间为 200，产生的随机数对 300 取余，那么落在第一个池的概率为 1/3，第二个池的概率为 2/3，这样当新增存储池时，对象总是倾向于快速填满新的空间。

```go
func (z *erasureServerPools) PutObject(ctx context.Context, bucket string, object string, data *PutObjReader, opts ObjectOptions) (ObjectInfo, error) {
   // Validate put object input args.
   if err := checkPutObjectArgs(ctx, bucket, object, z); err != nil {
      return ObjectInfo{}, err
   }

   object = encodeDirObject(object)

   if z.SinglePool() {
      ...
      return z.serverPools[0].PutObject(ctx, bucket, object, data, opts)
   }
   ...

   idx, err := z.getPoolIdxNoLock(ctx, bucket, object, data.Size())

   // Overwrite the object at the right pool
   return z.serverPools[idx].PutObject(ctx, bucket, object, data, opts)
}
```



## 选择 Set

Set 的选择很简单，就是根据对象名字进行哈希计算，选择了 Set 之后，就可以对 Set 里的 Drive 进行真正的对象操作了

```go
func (s *erasureSets) PutObject(ctx context.Context, bucket string, object string, data *PutObjReader, opts ObjectOptions) 
(objInfo ObjectInfo, err error) {
   set := s.getHashedSet(object)
   auditObjectErasureSet(ctx, object, set)
   return set.PutObject(ctx, bucket, object, data, opts)
}

func (er erasureObjects) PutObject(ctx context.Context, bucket string, object string, data *PutObjReader, opts ObjectOptions) 
(objInfo ObjectInfo, err error) {
	return er.putObject(ctx, bucket, object, data, opts)
}
```



## 对象写入

putObject 流程如下：

首先获取 set 组所有磁盘的接口，这是肯定的，因为读写操作最后都是在磁盘上完成的。

```go
storageDisks := er.getDisks()
```

然后计算 EC 比例：

-   首先校验数目初始化为磁盘的一半，即纠删比例 N：N，如果参数选项明确要求最大比例，那么就不进行接下来的计算了。
-   如果用户传了 storage class，那么就获取对应的 EC 比例，没有获取到的话就先设置为之前计算的默认值。
-   遍历所有磁盘接口，如果磁盘不可用、不在线、获取不到信息等，就增加校验数目。比如某个 set 包含 16 块盘，12：4 的纠删比，假如坏了 2 块盘，现在就变成了 10（16-6）：6（4+2），这里怎么理解呢？我理解就是有坏盘之后，MinIO 会提高 EC 比例来保证新写入数据的安全性，比如坏了 2 块盘，之前写入的对象只允许再坏两块盘，但是新写入的数据纠删比例变成了 10：6，也就是说新写入的数据还可以允许坏四块盘，因为 MinIO 不像 Ceph 在故障后有数据重平衡，所以它选择了这样的方式来提高数据可靠性。
-   如果校验数目大于 N / 2 了，那么就取 N / 2，因为校验数据总数不会超过总盘数的一半。
-   知道了校验盘数目，那么也就知道了数据盘数目并设置写配额，一般来说配额就是数据盘数目，如果是 N：N 这样子的，那么就设置为 dataBlocks + 1。

```go
parityDrives := len(storageDisks) / 2
if !opts.MaxParity {
    // Get parity and data drive count based on storage class metadata
    parityDrives = globalStorageClass.GetParityForSC(opts.UserDefined[xhttp.AmzStorageClass])
    if parityDrives <= 0 {
        parityDrives = er.defaultParityCount
    }

    // If we have offline disks upgrade the number of erasure codes for this object.
    parityOrig := parityDrives

    atomicParityDrives := uatomic.NewInt64(0)
    // Start with current parityDrives
    atomicParityDrives.Store(int64(parityDrives))

    // 如果检测到异常磁盘，则增加校验盘数目
    var wg sync.WaitGroup
    for _, disk := range storageDisks {
        if disk == nil {
            atomicParityDrives.Inc()
            continue
        }
        if !disk.IsOnline() {
            atomicParityDrives.Inc()
            continue
        }
        wg.Add(1)
        go func(disk StorageAPI) {
            defer wg.Done()
            di, err := disk.DiskInfo(ctx)
            if err != nil || di.ID == "" {
                atomicParityDrives.Inc()
            }
        }(disk)
    }
    wg.Wait()

    // 因故障磁盘 EC 升级后校验盘数目不能超过总数的 1/2
    parityDrives = int(atomicParityDrives.Load())
    if parityDrives >= len(storageDisks)/2 {
        parityDrives = len(storageDisks) / 2
    }
    if parityOrig != parityDrives {
        opts.UserDefined[minIOErasureUpgraded] = strconv.Itoa(parityOrig) + "->" + strconv.Itoa(parityDrives)
    }
}
// 计算数据块数目
dataDrives := len(storageDisks) - parityDrives

// 我们现在知道这个对象的数据和奇偶校验所需的块数
writeQuorum := dataDrives
if dataDrives == parityDrives {
    writeQuorum++
}
```

接着分配元数据 FileInfo，hashOrder 根据随机数决定哪些磁盘放数据，哪些磁盘放校验数据。

```go
func newFileInfo(object string, dataBlocks, parityBlocks int) (fi FileInfo) {
   fi.Erasure = ErasureInfo{
      Algorithm:    erasureAlgorithm,
      DataBlocks:   dataBlocks,
      ParityBlocks: parityBlocks,
      BlockSize:    blockSizeV2,
      Distribution: hashOrder(object, dataBlocks+parityBlocks),
   }
   return fi
}

func hashOrder(key string, cardinality int) []int {
	if cardinality <= 0 {
		// Returns an empty int slice for cardinality < 0.
		return nil
	}

	nums := make([]int, cardinality)
	keyCrc := crc32.Checksum([]byte(key), crc32.IEEETable)

	start := int(keyCrc % uint32(cardinality))
	for i := 1; i <= cardinality; i++ {
		nums[i-1] = 1 + ((start + i) % cardinality)
	}
	return nums
}
```

shuffleDisksAndPartsMetadata 根据 hashOrder 生成的序列重新洗牌，比如上面随机的顺序为 [0：2，1：3，2：4，3：5，4：6，5：1]，那么对原有的磁盘接口和元数据洗牌后 [1：0，2：1，3：2，4：3，5：4，0：5]

>[!WARNING]
>
>不过这里为啥不直接把第一个随机顺序初始化成 [0：1，1：2，2：3，3：4，4：5，5：0]，这样就直接得出洗牌后的 [1：0，2：1，3：2，4：3，5：4，0：5]
>
>在代码里生成随机数时先加 1，处理时再减 1 的妙处我倒是没能明白。

```go
func shuffleDisksAndPartsMetadata(disks []StorageAPI, partsMetadata []FileInfo, fi FileInfo) 
(shuffledDisks []StorageAPI, shuffledPartsMetadata []FileInfo) {
   ...
   // Shuffle slice xl metadata for expected distribution.
   for index := range partsMetadata {
      ...
      blockIndex := distribution[index]
      shuffledPartsMetadata[blockIndex-1] = partsMetadata[index]
      shuffledDisks[blockIndex-1] = disks[index]
   }
   return shuffledDisks, shuffledPartsMetadata
}
```

接下来就要开始纠删处理的部分了。NewErasure 生成一个 EC 存储对象，并初始化了 reedsolomon 纠删算法的方法，关于 reedsolomon 算法原理可以参考[Reed-Solomon编码算法](https://lu-xiaodai.gitee.io/#/posts/%E5%AD%98%E5%82%A8%E5%AE%89%E5%85%A8/Reed-Solomon%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95)，这里就不多赘述了。

```go
func NewErasure(ctx context.Context, dataBlocks, parityBlocks int, blockSize int64) (e Erasure, err error) {
   // Check the parameters for sanity now.
   if dataBlocks <= 0 || parityBlocks <= 0 {
      return e, reedsolomon.ErrInvShardNum
   }

   if dataBlocks+parityBlocks > 256 {
      return e, reedsolomon.ErrMaxShardNum
   }

   e = Erasure{
      dataBlocks:   dataBlocks,
      parityBlocks: parityBlocks,
      blockSize:    blockSize,
   }

   // Encoder when needed.
   var enc reedsolomon.Encoder
   var once sync.Once
   e.encoder = func() reedsolomon.Encoder {
      once.Do(func() {
         e, err := reedsolomon.New(dataBlocks, parityBlocks, reedsolomon.WithAutoGoroutines(int(e.ShardSize())))
         if err != nil {
            // Error conditions should be checked above.
            panic(err)
         }
         enc = e
      })
      return enc
   }
   return
}
```

因为在内存中处理 HTTP 传过来的对象，如果经常去分配释放内存，是很影响性能的。因此 MinIO 准备了缓存池，首先尝试为 I/O 获取缓冲区，如果没有则从池中返回，分配一个新的缓冲区并返回。

如果对象大小为 0，则分配一个字节来接收 EOF；如果对象大小大于 BlockSize，则只需要分配 BlockSize 大小的缓存就行了，因为后面计算纠删的时候是按 BlockSize 切割来计算的；如果小于 BlockSize，则分配对应 size 的缓存。

```go
var buffer []byte
switch size := data.Size(); {
case size == 0:
   buffer = make([]byte, 1) // Allocate atleast a byte to reach EOF
case size == -1:
   if size := data.ActualSize(); size > 0 && size < fi.Erasure.BlockSize {
      buffer = make([]byte, data.ActualSize()+256, data.ActualSize()*2+512)
   } else {
      buffer = er.bp.Get()
      defer er.bp.Put(buffer)
   }
case size >= fi.Erasure.BlockSize:
   buffer = er.bp.Get()
   defer er.bp.Put(buffer)
case size < fi.Erasure.BlockSize:
   // No need to allocate fully blockSizeV1 buffer if the incoming data is smaller.
   buffer = make([]byte, size, 2*size+int64(fi.Erasure.ParityBlocks+fi.Erasure.DataBlocks-1))
}

if len(buffer) > int(fi.Erasure.BlockSize) {
   buffer = buffer[:fi.Erasure.BlockSize]
}
```

Shard 分片是指最终每个拆分的数据块对象的大小，计算也很简单，ceilFrac 用于计算指定数据大小分成数据块数目后每个数据块的大小。比如一片数据大小为 437，有 6 个数据块，那么平均每个块要存 73 字节（437 / 6 = 72，无法整除）。

ShardFileSize 计算分片时分为了两部分，一部分根据 blockSize 用 ceilFrac 计算，这里是固定的，另一部分是 blockSize 不能整除下来的片段，这段 ceilFrac 单独计算再和前面的加起来就是每个 Shard 的大小。

```go
func (e *Erasure) ShardFileSize(totalLength int64) int64 {
	if totalLength == 0 {
		return 0
	}
	if totalLength == -1 {
		return -1
	}
	numShards := totalLength / e.blockSize
	lastBlockSize := totalLength % e.blockSize
	lastShardSize := ceilFrac(lastBlockSize, int64(e.dataBlocks))
	return numShards*e.ShardSize() + lastShardSize
}
```

对于小文件来说，MinIO 会将它和元数据一起存储，而不是分开两个文件单独存。smallFileThreshold 目前设置为 128 KB，也就是说在不带版本的情况下分片小于 128 KB，或者分片小于 16 KB 的情况下，都会分配 inlineBuffers 用于同时存储数据和元数据。

```go
var inlineBuffers []*bytes.Buffer
if shardFileSize >= 0 {
   if !opts.Versioned && shardFileSize < smallFileThreshold {
      inlineBuffers = make([]*bytes.Buffer, len(onlineDisks))
   } else if shardFileSize < smallFileThreshold/8 {
      inlineBuffers = make([]*bytes.Buffer, len(onlineDisks))
   }
} else {
   // If compressed, use actual size to determine.
   if sz := erasure.ShardFileSize(data.ActualSize()); sz > 0 {
      if !opts.Versioned && sz < smallFileThreshold {
         inlineBuffers = make([]*bytes.Buffer, len(onlineDisks))
      } else if sz < smallFileThreshold/8 {
         inlineBuffers = make([]*bytes.Buffer, len(onlineDisks))
      }
   }
}

for i, disk := range onlineDisks {
    if disk == nil {
        continue
    }

    if !disk.IsOnline() {
        continue
    }

    if len(inlineBuffers) > 0 {
        sz := shardFileSize
        if sz < 0 {
            sz = data.ActualSize()
        }
        inlineBuffers[i] = bytes.NewBuffer(make([]byte, 0, sz))
        writers[i] = newStreamingBitrotWriterBuffer(inlineBuffers[i], DefaultBitrotAlgorithm, erasure.ShardSize())
        continue
    }

    writers[i] = newBitrotWriter(disk, minioMetaTmpBucket, tempErasureObj, shardFileSize, DefaultBitrotAlgorithm, erasure.ShardSize())
}
```

在进行纠删计算前，如果数据大于 128 MB，MinIO 做了一些优化处理，这里分配了两段 buffer，这里的 buffer 可以循环使用，从 HTTP 读取数据后就发送到准备通道中，使用完毕后又放回重复通道，继续从 HTTP 读取数据，读取数据和计算 EC 是异步的过程，保证总有一个 buffer 保存了 input 的数据。

```go
toEncode := io.Reader(data)
if data.Size() > bigFileThreshold {
   // We use 2 buffers, so we always have a full buffer of input.
   bufA := er.bp.Get()
   bufB := er.bp.Get()
   defer er.bp.Put(bufA)
   defer er.bp.Put(bufB)
   ra, err := readahead.NewReaderBuffer(data, [][]byte{bufA[:fi.Erasure.BlockSize], bufB[:fi.Erasure.BlockSize]})
   if err == nil {
      toEncode = ra
      defer ra.Close()
   }
   logger.LogIf(ctx, err)
}
```

接下来就按 BlockSize 切割来计算纠删和 BirBot Hash 值（如果大小不到 BlockSize，那么就一次计算完毕了），这里只是把对象写入了临时文件或 inlineBuffers 里。

```go
func (e *Erasure) Encode(ctx context.Context, src io.Reader, writers []io.Writer, buf []byte, quorum int) (total int64, err error) {
   ...
   for {
      var blocks [][]byte
      n, err := io.ReadFull(src, buf)
      ...
      // We take care of the situation where if n == 0 and total == 0 by creating empty data and parity files.
      blocks, err = e.EncodeData(ctx, buf[:n])
      ...

      if err = writer.Write(ctx, blocks); err != nil {
         ...
      }
      total += int64(n)
      if eof {
         break
      }
   }
   return total, nil
}
```

在更新相关元数据之后，renameData 将对象重命名，完成原子写入。RenameData 操作是并发的，允许 `len(er.getDisks()) - writeQuorum` 个操作出错，否则返回错误。

```go
func renameData(ctx context.Context, disks []StorageAPI, srcBucket, srcEntry string, metadata []FileInfo, 
                dstBucket, dstEntry string, writeQuorum int) ([]StorageAPI, error) {
	defer NSUpdated(dstBucket, dstEntry)

	g := errgroup.WithNErrs(len(disks))

	fvID := mustGetUUID()
	for index := range disks {
		metadata[index].SetTierFreeVersionID(fvID)
	}
	// Rename file on all underlying storage disks.
	for index := range disks {
		index := index
		g.Go(func() error {
			if disks[index] == nil {
				return errDiskNotFound
			}
			// Pick one FileInfo for a disk at index.
			fi := metadata[index]
			// Assign index when index is initialized
			if fi.Erasure.Index == 0 {
				fi.Erasure.Index = index + 1
			}

			if fi.IsValid() {
				return disks[index].RenameData(ctx, srcBucket, srcEntry, fi, dstBucket, dstEntry)
			}
			return errFileCorrupt
		}, index)
	}

	// Wait for all renames to finish.
	errs := g.Wait()

	// We can safely allow RenameData errors up to len(er.getDisks()) - writeQuorum
	// otherwise return failure. Cleanup successful renames.
	err := reduceWriteQuorumErrs(ctx, errs, objectOpIgnoredErrs, writeQuorum)
	return evalDisks(disks, errs), err
}
```

在这次上传过程中，无论一个磁盘是最初还是过程中变成离线的，都要把它送到 MRF 列表中，这用于快速恢复，MinIO 会定期扫描磁盘状态，但是这里在访问时既然已经发现了磁盘故障自然是要去修复而不是等到扫描时再修复。

```go
if !opts.Speedtest {
   // Whether a disk was initially or becomes offline
   // during this upload, send it to the MRF list.
   for i := 0; i < len(onlineDisks); i++ {
      if onlineDisks[i] != nil && onlineDisks[i].IsOnline() {
         continue
      }

      er.addPartial(bucket, object, fi.VersionID, fi.Size)
      break
   }
}
```

最后统计在线磁盘数（如果没有执行到最后，online 则会是初值 0，很明显不等条件会成立），表示在写入过程中有磁盘状态发生变化，此时当作写入出现问题，在退出时删除失败情况下的临时文件，这里也不是真的删除，而是把要删除的文件移动到 `.minio.sys/tmp/.trash` 下后续一并处理。

```go
defer func() {
   if online != len(onlineDisks) {
      er.renameAll(context.Background(), minioMetaTmpBucket, tempObj)
   }
}()

func (er erasureObjects) renameAll(ctx context.Context, bucket, prefix string) {
	var wg sync.WaitGroup
	for _, disk := range er.getDisks() {
		if disk == nil {
			continue
		}
		wg.Add(1)
		go func(disk StorageAPI) {
			defer wg.Done()
			disk.RenameFile(ctx, bucket, prefix, minioMetaTmpDeletedBucket, mustGetUUID())
		}(disk)
	}
	wg.Wait()
}
```







---

# GET 流程

首先我们找到 API 入口：

```go
router.Methods(http.MethodGet).Path("/{object:.+}").HandlerFunc(
   collectAPIStats("getobject", maxClients(gz(httpTraceHdrs(api.GetObjectHandler)))))
```

API 层这里就不详细分析了，我们直接去看对象层的 GetObjectNInfo 接口。

```go
func (api objectAPIHandlers) GetObjectHandler(w http.ResponseWriter, r *http.Request) {
   ...
   objectAPI := api.ObjectAPI()
   ...

   vars := mux.Vars(r)
   bucket := vars["bucket"]
   object, err := unescapePath(vars["object"])
   ...

   if r.Header.Get(xMinIOExtract) == "true" && strings.Contains(object, archivePattern) {
      api.getObjectInArchiveFileHandler(ctx, objectAPI, bucket, object, w, r)
   } else {
      api.getObjectHandler(ctx, objectAPI, bucket, object, w, r)
   }
}

func (api objectAPIHandlers) getObjectHandler(ctx context.Context, objectAPI ObjectLayer, bucket, object string, 
                                              w http.ResponseWriter, r *http.Request) {
	...
    getObjectNInfo := objectAPI.GetObjectNInfo
    ...
    gr, err := getObjectNInfo(ctx, bucket, object, rs, r.Header, readLock, opts)
    objInfo := gr.ObjInfo
    // 自动删除到期的 object/version
	if lc, err := globalLifecycleSys.Get(bucket); err == nil {
		...
	}

	// filter object lock metadata if permission does not permit
	...

	// Set encryption response headers
	...

	// Set Parts Count Header
	...

	// Write object content to response body
	...

	// Notify object accessed via a GET request.
    ...
}
```

GetObjectNInfo 也是一个选池的过程，和 PUT 类似，如果只有一个池，就调用该池的接口，否则要去找到对象在哪个池里。

```go
func (z *erasureServerPools) GetObjectNInfo(ctx context.Context, bucket, object string, rs *HTTPRangeSpec, 
                                            h http.Header, lockType LockType, opts ObjectOptions) 
(gr *GetObjectReader, err error) {
   ...

   if z.SinglePool() {
      return z.serverPools[0].GetObjectNInfo(ctx, bucket, object, rs, h, lockType, opts)
   }

   var unlockOnDefer bool
   nsUnlocker := func() {}
   defer func() {
      if unlockOnDefer {
         nsUnlocker()
      }
   }()

   // Acquire lock
   if lockType != noLock {
      ...
   }

   ...
   objInfo, zIdx, err := z.getLatestObjectInfoWithIdx(ctx, bucket, object, opts)
   ...

   lockType = noLock // do not take locks at lower levels for GetObjectNInfo()
   return z.serverPools[zIdx].GetObjectNInfo(ctx, bucket, object, rs, h, lockType, opts)
}
```

getLatestObjectInfoWithIdx 从多个池中返回最新的对象信息（这个函数的存在是为了防止向两个池重复写入，这个函数也返回最新对象存在的附加索引，用于启动GetObject流。它首先向每个池都调用 `pool.GetObjectInfo` 方法，这会返回对象的元数据。然后对返回结果进行排序，这里防御性处理任何可能已经创建的重复内容。

```go
func (z *erasureServerPools) getLatestObjectInfoWithIdx(ctx context.Context, bucket, object string, opts ObjectOptions) (ObjectInfo, int, error) {
   object = encodeDirObject(object)
   results := make([]struct {
      zIdx int
      oi   ObjectInfo
      err  error
   }, len(z.serverPools))
   // 向每个池获取该对象的元数据
   var wg sync.WaitGroup
   for i, pool := range z.serverPools {
      wg.Add(1)
      go func(i int, pool *erasureSets) {
         defer wg.Done()
         results[i].zIdx = i
         results[i].oi, results[i].err = pool.GetObjectInfo(ctx, bucket, object, opts)
      }(i, pool)
   }
   wg.Wait()

   // 排序，始终选择最新的对象服务
   sort.Slice(results, func(i, j int) bool {
      a, b := results[i], results[j]
      if a.oi.ModTime.Equal(b.oi.ModTime) {
         // On tiebreak, select the lowest zone index.
         return a.zIdx < b.zIdx
      }
      return a.oi.ModTime.After(b.oi.ModTime)
   })

   for _, res := range results {
      err := res.err
      if err == nil {
         return res.oi, res.zIdx, nil
      }
      ...
   }

   ...
}
```

接下来我们就去对象所处的池里取出对象的内容，一个对象在一个池里所属的 set 组是 hash 确定的。

```go
func (s *erasureSets) GetObjectNInfo(ctx context.Context, bucket, object string, rs *HTTPRangeSpec, h http.Header, 
                                     lockType LockType, opts ObjectOptions) (gr *GetObjectReader, err error) {
   set := s.getHashedSet(object)
   auditObjectErasureSet(ctx, object, set)
   return set.GetObjectNInfo(ctx, bucket, object, rs, h, lockType, opts)
}
```

最后 GetObjectNInfo 读取对象数据。





